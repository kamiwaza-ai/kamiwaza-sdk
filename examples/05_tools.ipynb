{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92495f50-72d6-4181-863a-64578fb2cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kamiwaza_sdk import KamiwazaClient as kz\n",
    "import openai\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45925bd1-e294-4e6f-a6bd-a8446e6ba5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kz(\"http://localhost:7777/api/\")\n",
    "client.serving.list_active_deployments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e29f9b9-b838-476b-b974-6ff466fc6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = client.openai.get_client('Qwen3-8B-GGUF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1551c-8a67-48cc-9f87-671ed5c1998c",
   "metadata": {},
   "source": [
    "## Define the runtime tool & its JSON schema\n",
    "We expose a dummy weather function so the demo is entirely selfâ€‘contained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8853b-8024-475f-a696-1b9b0072195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import json, time\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class WeatherParams(BaseModel):\n",
    "    city: str = Field(..., description=\"City name\")\n",
    "    state: str = Field(..., description=\"US state abbreviation, e.g. CA\")\n",
    "    unit: str = Field(..., description=\"Either 'celsius' or 'fahrenheit'\")\n",
    "\n",
    "def get_current_weather(city: str, state: str, unit: str) -> Dict[str, Any]:\n",
    "    \"\"\"Pretend weather service so we don't need a real API call.\"\"\"\n",
    "    fake_temp_f = 100  # you could randomise this or call a real API\n",
    "    temp = fake_temp_f if unit == \"fahrenheit\" else round((fake_temp_f - 32) * 5 / 9)\n",
    "    time.sleep(0.3)  # small pause so the streaming looks lively\n",
    "    return {\n",
    "        \"temperature\": temp,\n",
    "        \"unit\": unit,\n",
    "        \"city\": city,\n",
    "        \"state\": state,\n",
    "        \"description\": \"clear skies with a light breeze\",\n",
    "    }\n",
    "\n",
    "weather_tool_schema = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given US city\",\n",
    "        \"parameters\": WeatherParams.schema(),  # Pydantic â†’ JSON Schema\n",
    "    },\n",
    "}\n",
    "\n",
    "tools = [weather_tool_schema]\n",
    "\n",
    "local_tool_registry = {\"get_current_weather\": get_current_weather}\n",
    "print(\"âœ… Tool registered â€” ready for the chat call\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821199d-66e1-4d37-add3-4e5f44ae810b",
   "metadata": {},
   "source": [
    "##Â Kick off a streaming chat with tool support\n",
    "The system prompt asks the model to *think in public* using `<thinking>` \n",
    "tags, then decide whether to call the weather tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6377a2-d5e8-4ed0-92f4-a694266e5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a helpful assistant. When you receive tool outputs, \"\n",
    "            \"always provide a complete and helpful final answer to the user's question. \"\n",
    "            \"Be conversational and provide practical advice.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"should i wear short sleeves or long sleeves in nyc today?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“¡ **Streaming** â€” watch the tokens, tool call, and final answer arrive\\n\")\n",
    "\n",
    "# First API call with tools\n",
    "response_stream = openai_client.chat.completions.create(\n",
    "    model=\"model\", \n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",  # Let the model decide when to use tools\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Track tool calls as they stream in\n",
    "tool_calls = []\n",
    "current_tool_call = None\n",
    "accumulated_content = \"\"\n",
    "\n",
    "for chunk in response_stream:\n",
    "    if not chunk.choices:\n",
    "        continue\n",
    "    \n",
    "    choice = chunk.choices[0]\n",
    "    delta = choice.delta\n",
    "    \n",
    "    # Handle reasoning content (if your model supports it)\n",
    "    if hasattr(delta, \"reasoning_content\") and delta.reasoning_content:\n",
    "        print(\"\\033[36m\" + delta.reasoning_content + \"\\033[0m\", end=\"\", flush=True)\n",
    "    \n",
    "    # Handle regular content\n",
    "    if delta.content:\n",
    "        accumulated_content += delta.content\n",
    "        print(delta.content, end=\"\", flush=True)\n",
    "    \n",
    "    # Handle tool calls\n",
    "    if delta.tool_calls:\n",
    "        for tc_delta in delta.tool_calls:\n",
    "            # Start a new tool call\n",
    "            if tc_delta.index == 0 and not current_tool_call:\n",
    "                current_tool_call = {\n",
    "                    \"id\": tc_delta.id or f\"call_{uuid.uuid4().hex[:8]}\",\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tc_delta.function.name,\n",
    "                        \"arguments\": \"\"\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "            # Accumulate arguments\n",
    "            if tc_delta.function.arguments:\n",
    "                current_tool_call[\"function\"][\"arguments\"] += tc_delta.function.arguments\n",
    "\n",
    "# Execute tool calls if any\n",
    "if choice.finish_reason == \"tool_calls\" and current_tool_call:\n",
    "    print(f\"\\n\\nðŸ”§ Model requested tool: {current_tool_call['function']['name']}\")\n",
    "    \n",
    "    # Parse arguments and execute\n",
    "    args = json.loads(current_tool_call[\"function\"][\"arguments\"])\n",
    "    print(f\"   with args: {args}\")\n",
    "    \n",
    "    tool_name = current_tool_call[\"function\"][\"name\"]\n",
    "    tool_result = local_tool_registry[tool_name](**args)\n",
    "    print(f\"ðŸ”§ Tool response: {tool_result}\")\n",
    "    \n",
    "    # Add the assistant's tool call message\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": accumulated_content if accumulated_content else None,\n",
    "        \"tool_calls\": [current_tool_call]\n",
    "    })\n",
    "    \n",
    "    # Add the tool response\n",
    "    messages.append({\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": current_tool_call[\"id\"],\n",
    "        \"name\": tool_name,\n",
    "        \"content\": json.dumps(tool_result)\n",
    "    })\n",
    "    \n",
    "    # Make a follow-up call to get the final answer\n",
    "    print(\"\\n\\nðŸ’­ Getting final answer...\\n\")\n",
    "    \n",
    "    follow_stream = openai_client.chat.completions.create(\n",
    "        model=\"model\",\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        # No tools this time - we want a final answer\n",
    "    )\n",
    "    \n",
    "    # Stream the final response\n",
    "    for follow_chunk in follow_stream:\n",
    "        if not follow_chunk.choices:\n",
    "            continue\n",
    "        \n",
    "        follow_choice = follow_chunk.choices[0]\n",
    "        follow_delta = follow_choice.delta\n",
    "        \n",
    "        # Handle reasoning content\n",
    "        if hasattr(follow_delta, \"reasoning_content\") and follow_delta.reasoning_content:\n",
    "            print(\"\\033[36m\" + follow_delta.reasoning_content + \"\\033[0m\", end=\"\", flush=True)\n",
    "        \n",
    "        # Handle final answer content\n",
    "        if follow_delta.content:\n",
    "            print(follow_delta.content, end=\"\", flush=True)\n",
    "    \n",
    "    print(\"\\n\\nðŸŽ‰ **Done!**\")\n",
    "\n",
    "# If no tool was called, we're done\n",
    "elif choice.finish_reason == \"stop\":\n",
    "    print(\"\\n\\nðŸŽ‰ **Done!** (No tool calls needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f18a82-683b-4860-8b3e-a7e6b27988b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

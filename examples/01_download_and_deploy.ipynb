{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c32fc3a-bb41-468b-a3b4-6d4629d17657",
   "metadata": {},
   "source": [
    "# Model Download and Deployment with Kamiwaza SDK\n",
    "\n",
    "This notebook demonstrates how to download and deploy models using the Kamiwaza SDK. We'll walk through the complete process step-by-step:\n",
    "\n",
    "1. Searching for models\n",
    "2. Downloading model files\n",
    "3. Deploying the model\n",
    "4. Using the model with the OpenAI compatible interface\n",
    "5. Stopping the model deployment\n",
    "\n",
    "In this example, we're using a small language model ([Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF)), but the same process works for any supported model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39734d24-c68b-4b39-99c6-e7e2eae74fbe",
   "metadata": {},
   "source": [
    "## Initialize the Kamiwaza Client\n",
    "\n",
    "First, we initialize the client by connecting to our Kamiwaza server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1ba97-29fa-44c1-b8cd-1a547ad17dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kamiwaza_sdk import kamiwaza_sdk as kz\n",
    "\n",
    "# Initialize the client\n",
    "client = kz(\"http://localhost:7777/api/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ac8454-bde3-417f-a5f5-37b315470362",
   "metadata": {},
   "source": [
    "## Search for a Model\n",
    "\n",
    "Let's search for a specific model from Hugging Face. We use the `search_models` method with the repository ID and set `exact=True` to find an exact match.\n",
    "\n",
    "The search results show:\n",
    "- Model name and repository ID\n",
    "- Available files and their types\n",
    "- Available quantization levels (fp16, q2_k, q3_k, etc.)\n",
    "- Download status information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b68f67-d90c-45e9-98b0-62586cea8c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_repo = 'Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF'\n",
    "client.models.search_models(hf_repo, exact = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e0a2b-9619-4da8-8c16-feb4233735d4",
   "metadata": {},
   "source": [
    "## Download the Model Files\n",
    "\n",
    "Now we'll initiate the model download using `initiate_model_download`. \n",
    "\n",
    "By default, this downloads the best quantization for your hardware. You can specify a particular quantization level by adding a parameter like:\n",
    "\n",
    "```python\n",
    "client.models.initiate_model_download(hf_repo, quantization=\"q4_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9ec9a-9548-4984-bb96-ffbc43ba5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.models.initiate_model_download(hf_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08a72ca-0008-4762-b2ec-e0e60eaaa54d",
   "metadata": {},
   "source": [
    "## Check Download Status\n",
    "\n",
    "After initiating the download, we can check its status to see the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842b070-889f-4d9c-afe1-1a4a4343de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.models.check_download_status(hf_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be211c-25d1-4daf-9258-935eb2fef9ac",
   "metadata": {},
   "source": [
    "## Wait for Download Completion\n",
    "\n",
    "Instead of repeatedly checking the status, we can use the `wait_for_download` method to wait until all downloads are complete. This method provides progress updates and a summary once the download finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373c65d-1b08-43fe-a09f-ae4b72ebac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.models.wait_for_download(hf_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597abe60-b509-4481-90de-6f2e509dff79",
   "metadata": {},
   "source": [
    "## Deploy the Model\n",
    "\n",
    "Once the model is downloaded, we can deploy it using `deploy_model`. This method prepares the model for inference and returns a deployment ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75794ee-9939-47f8-ad99-66a61ea9d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.serving.deploy_model(repo_id=hf_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be96d3-3241-4066-a151-1e80accbcd0d",
   "metadata": {},
   "source": [
    "## List Active Deployments\n",
    "\n",
    "We can view all active model deployments to confirm our model is running. The output shows:\n",
    "- Deployment ID and model ID\n",
    "- Model name\n",
    "- Status (DEPLOYED, STARTING, etc.)\n",
    "- Instance information\n",
    "- The endpoint URL for making inference requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca50802-e0b5-4f9c-a060-a5e3fbd6aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.serving.list_active_deployments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51aa47-7ee2-46d0-8784-c37c3052f02e",
   "metadata": {},
   "source": [
    "## Use the OpenAI Compatible Interface\n",
    "\n",
    "Kamiwaza provides an OpenAI-compatible interface, making it easy to use familiar tools with your deployed models. We create an OpenAI client using the `get_client` method.\n",
    "\n",
    "Now we can use the standard OpenAI API patterns to interact with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24224749-89cc-4597-8e9c-19cb60a019e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = client.openai.get_client(repo_id=hf_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735aaed0-0cc1-4b68-80e6-d797d0f37d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming chat completion\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How many r's are in the word 'strawberry'? ONLY RESPOND WITH A SINGLE NUMBER\"}\n",
    "    ],\n",
    "    model=\"model\",\n",
    "    stream=True \n",
    ")\n",
    "\n",
    "# display the stream\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecfb25-8217-492c-a889-f4fc74370e95",
   "metadata": {},
   "source": [
    "## Stop the Model Deployment\n",
    "\n",
    "When we're done using the model, we can stop the deployment to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa10dcd-f3a9-4d11-aae9-499ef693d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.serving.stop_deployment(repo_id=hf_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e53463-b242-47e0-b762-a7b4d1a20ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.serving.list_active_deployments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a882e1e6-e1ca-4f5e-876a-fbbbc896faf2",
   "metadata": {},
   "source": [
    "# Model Evaluation: Counting the 'r's in \"strawberry\"\n",
    "\n",
    "This notebook demonstrates how to use Kamiwaza SDK to evaluate multiple language models on a simple counting task. We'll deploy three different models, ask each one to count the number of 'r's in the word \"strawberry\" (the correct answer is 3), and compare their responses.\n",
    "\n",
    "This approach shows how Kamiwaza can be used for automated model evaluation and benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b0c8c-9a14-4351-a32e-cd4cb426beb3",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, we'll initialize the Kamiwaza client and define the list of models we want to evaluate. We'll also create a directory to store our evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5319a-1fb3-461e-be10-5983a1d342ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kamiwaza_sdk import kamiwaza_sdk as kz\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa5fe6-792c-4d85-9d0f-7272b7293f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Kamiwaza client\n",
    "client = kz(\"http://localhost:7777/api/\")\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = [\n",
    "    \"Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct-GGUF\",\n",
    "    \"bartowski/Llama-3-8B-Instruct-Coder-v2-GGUF\"\n",
    "]\n",
    "\n",
    "os.makedirs(\"eval_results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4c369-e9ce-4409-a8cd-e37e8359e60b",
   "metadata": {},
   "source": [
    "## 2. Run Evaluations\n",
    "\n",
    "Next, we'll loop through each model and:\n",
    "1. Download and deploy the model (if not already downloaded)\n",
    "2. Wait for the model to fully initialize\n",
    "3. Run 50 evaluation queries asking \"How many r's are in the word 'strawberry'?\"\n",
    "4. Save the results to a JSON file\n",
    "5. Stop the deployment before moving to the next model\n",
    "\n",
    "This automated process allows us to systematically test each model with the same prompt and collect their responses for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b295c93-9f20-4d0c-917f-520c7ec2ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create output directory for results\n",
    "os.makedirs(\"eval_results\", exist_ok=True)\n",
    "\n",
    "# Number of evaluation runs per model\n",
    "num_runs = 500\n",
    "\n",
    "# Evaluation prompt\n",
    "prompt = \"How many r's are in the word 'strawberry'? ONLY RESPOND WITH A SINGLE NUMBER\"\n",
    "\n",
    "# Track overall results\n",
    "all_results = {}\n",
    "\n",
    "for model_repo in models:\n",
    "    print(f\"Evaluating model: {model_repo}\")\n",
    "    model_name = model_repo.split('/')[-1]\n",
    "    \n",
    "    # Download and deploy the model\n",
    "    print(f\"Downloading and deploying {model_name}...\")\n",
    "    result = client.models.download_and_deploy_model(model_repo)\n",
    "    deployment_id = result.get('deployment_id')\n",
    "    \n",
    "    if not deployment_id:\n",
    "        print(f\"Failed to deploy {model_name}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Fixed wait time to ensure model is fully loaded\n",
    "    wait_time = 15  # seconds\n",
    "    print(f\"Waiting {wait_time} seconds for model to fully initialize...\")\n",
    "    time.sleep(wait_time)\n",
    "    \n",
    "    # Create OpenAI client for the model\n",
    "    openai_client = client.openai.get_client(repo_id=model_repo)\n",
    "    \n",
    "    # Run evaluations and collect responses\n",
    "    model_results = []\n",
    "    print(f\"Running {num_runs} evaluations...\")\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = openai_client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                model=\"model\",\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            \n",
    "            result = {\n",
    "                \"run_id\": i,\n",
    "                \"response\": response.choices[0].message.content.strip(),\n",
    "                \"time_seconds\": end_time - start_time\n",
    "            }\n",
    "            model_results.append(result)\n",
    "            \n",
    "            print(f\"Run {i+1}/{num_runs}: Response = '{result['response']}' ({result['time_seconds']:.2f}s)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on run {i+1}: {str(e)}\")\n",
    "            result = {\n",
    "                \"run_id\": i,\n",
    "                \"response\": \"ERROR\",\n",
    "                \"time_seconds\": time.time() - start_time,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            model_results.append(result)\n",
    "    \n",
    "    # Save the results to a file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"eval_results/{model_name}_{timestamp}.json\"\n",
    "    \n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(model_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {filename}\")\n",
    "    all_results[model_name] = {\n",
    "        \"filename\": filename,\n",
    "        \"results\": model_results\n",
    "    }\n",
    "    \n",
    "    # Stop the deployment\n",
    "    print(f\"Stopping deployment for {model_name}...\")\n",
    "    client.serving.stop_deployment(repo_id=model_repo)\n",
    "\n",
    "# Save the overall results\n",
    "with open(\"eval_results/all_models_summary.json\", \"w\") as f:\n",
    "    # We'll save just the filenames in the summary to avoid duplicating all the data\n",
    "    summary = {k: {\"filename\": v[\"filename\"]} for k, v in all_results.items()}\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"All evaluations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1554402a-b0fb-4646-8104-7f9ed706f6cc",
   "metadata": {},
   "source": [
    "## 3. Display Results\n",
    "\n",
    "Now we'll read the saved results and display a summary for each model, including:\n",
    "- Total number of evaluation runs\n",
    "- Average response time\n",
    "- Distribution of responses\n",
    "- Accuracy (percentage of correct answers)\n",
    "\n",
    "This gives us a quick overview of how each model performed on our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20d582-61ae-442f-9f06-f7b09138ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Get all result files\n",
    "result_files = [f for f in os.listdir(\"eval_results\") if f.endswith(\".json\") and f != \"all_models_summary.json\"]\n",
    "\n",
    "for file in result_files:\n",
    "    print(f\"\\n=== Results from {file} ===\")\n",
    "    with open(f\"eval_results/{file}\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    # Count occurrences of each response\n",
    "    responses = {}\n",
    "    for item in data:\n",
    "        response = item[\"response\"]\n",
    "        if response not in responses:\n",
    "            responses[response] = 0\n",
    "        responses[response] += 1\n",
    "    \n",
    "    # Calculate average response time\n",
    "    avg_time = sum(item[\"time_seconds\"] for item in data) / len(data)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"Total runs: {len(data)}\")\n",
    "    print(f\"Average response time: {avg_time:.2f} seconds\")\n",
    "    print(\"Response distribution:\")\n",
    "    for response, count in responses.items():\n",
    "        percentage = (count / len(data)) * 100\n",
    "        print(f\"  '{response}': {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Check for correct answers (3)\n",
    "    correct = sum(1 for item in data if item[\"response\"] == \"3\")\n",
    "    accuracy = (correct / len(data)) * 100\n",
    "    print(f\"Accuracy (correct answer is '3'): {correct}/{len(data)} ({accuracy:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d9bc7-1665-4534-89ed-aba48dfea12f",
   "metadata": {},
   "source": [
    "## 4. Visualize Response Distribution\n",
    "\n",
    "Finally, we'll create a visualization comparing how each model responded to our question across all trials.\n",
    "\n",
    "The chart shows the percentage distribution of answers from each model to the prompt \"How many r's are in strawberry?\" (the correct answer is 3). \n",
    "\n",
    "Qwen2.5-7B-Instruct-GGUF shows remarkably consistent performance with 96% correct answers, while the smaller Qwen2.5-Coder-0.5B-Instruct model and Llama-3-8B-Instruct show more varied responses. This simple evaluation demonstrates how Kamiwaza can be used to quickly benchmark multiple models on the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1e097-5e7f-4de7-865d-64f027c9b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Get all result files\n",
    "result_files = [f for f in os.listdir(\"eval_results\") if f.endswith(\".json\") and f != \"all_models_summary.json\"]\n",
    "\n",
    "# Prepare data for visualization\n",
    "model_names = []\n",
    "response_distributions = []\n",
    "\n",
    "for file in result_files:\n",
    "    model_name = file.split('_')[0]\n",
    "    model_names.append(model_name)\n",
    "    \n",
    "    with open(f\"eval_results/{file}\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Get response distribution\n",
    "    responses = {}\n",
    "    for item in data:\n",
    "        response = item[\"response\"]\n",
    "        if response not in responses:\n",
    "            responses[response] = 0\n",
    "        responses[response] += 1\n",
    "    response_distributions.append(responses)\n",
    "\n",
    "# Plot response distributions\n",
    "plt.figure(figsize=(14, 8))\n",
    "width = 0.2\n",
    "all_responses = sorted(set(resp for dist in response_distributions for resp in dist.keys()))\n",
    "x = np.arange(len(all_responses))\n",
    "\n",
    "for i, (model, dist) in enumerate(zip(model_names, response_distributions)):\n",
    "    values = [dist.get(resp, 0) for resp in all_responses]\n",
    "    percentages = [v / sum(values) * 100 for v in values]\n",
    "    plt.bar(x + (i - len(model_names)/2 + 0.5) * width, percentages, width, label=model)\n",
    "\n",
    "plt.ylabel('Percentage of Responses (%)', fontsize=12)\n",
    "plt.title('Model Response Distribution: \"How many r\\'s are in strawberry?\"', fontsize=16)\n",
    "plt.xticks(x, all_responses, fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.ylim(0, 105)  # Leave room for percentage labels\n",
    "\n",
    "# Add percentage labels on top of bars\n",
    "for i, (model, dist) in enumerate(zip(model_names, response_distributions)):\n",
    "    values = [dist.get(resp, 0) for resp in all_responses]\n",
    "    percentages = [v / sum(values) * 100 for v in values]\n",
    "    for j, pct in enumerate(percentages):\n",
    "        if pct > 0:  # Only add labels to non-zero bars\n",
    "            plt.text(\n",
    "                x[j] + (i - len(model_names)/2 + 0.5) * width, \n",
    "                pct + 2, \n",
    "                f\"{pct:.0f}%\", \n",
    "                ha='center',\n",
    "                fontsize=10\n",
    "            )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ac800-442b-4c2c-8b5a-24319cadc4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

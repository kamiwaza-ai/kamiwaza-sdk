{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f2cc62-a32f-4bb8-8a99-e9cc0d331fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install kamiwaza client sdk\n",
    "! cd ../../kamiwaza-sdk && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f95a77ff-15b9-4d43-b29b-f13bef59a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kamiwaza_client import KamiwazaClient\n",
    "import platform\n",
    "import re\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import openai\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize the client\n",
    "client = KamiwazaClient(\"http://localhost:7777/api/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e5bd6-0af5-47a6-a55a-8f9eac885748",
   "metadata": {},
   "source": [
    "## First check if we have any models deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad4fba-ba13-4d5e-b5f2-574b0cacff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = client.serving.list_deployments()\n",
    "if len(deployments) == 0:\n",
    "    print('No models deployed')\n",
    "else:\n",
    "    print(deployments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b43eec-d7b1-4f1a-82ef-f803a5e135ce",
   "metadata": {},
   "source": [
    "## Next, lets check if we have any models downloaded that we can deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e324b01-799c-418a-ba2d-4d7a697814e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_models = client.models.list_model_files()\n",
    "if len(downloaded_models) == 0:\n",
    "    print('No model files downloaded')\n",
    "else:\n",
    "    print(downloaded_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f480b-99c0-4530-9dc3-0049b95807fe",
   "metadata": {},
   "source": [
    "# We do not have any models deployed or downloaded, lets get a new model, download it, and deploy it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb71a4b-7268-4f29-a1b7-dfd89d44554a",
   "metadata": {},
   "source": [
    "# 1. Let's Download a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb45f5-d170-4af3-8d7a-7cb413ac68a6",
   "metadata": {},
   "source": [
    "### 1.1. Search for Qwen2.7-7B-Instruct on Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c488b389-a7f5-4b1d-8139-4a937e4dff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b850643-4339-4854-b962-97341e39ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.search_models(model_name)\n",
    "for m in range(len(models)):\n",
    "    print(f'model index: {m}')\n",
    "    print(models[m])\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cac17b2-37fa-4061-b6d6-d55c62dd892a",
   "metadata": {},
   "source": [
    "### 1.2. We have a helper function to filter these models to the ones compatible with our OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7b684-ab05-4d30-9c3f-afb9a8c18982",
   "metadata": {},
   "outputs": [],
   "source": [
    "compatible_models = client.models.filter_compatible_models(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "print(\"Compatible models:\")\n",
    "for model_info in compatible_models:\n",
    "    print(f\"Model: {model_info['model'].name}\")\n",
    "    print(f\"Repo Id: {model_info['model'].repo_modelId}\")\n",
    "    print(\"Compatible files:\")\n",
    "    for file in model_info['files']:\n",
    "        print(f\"- {file.name}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08935889-8258-485a-88f9-aa1bcc4c3380",
   "metadata": {},
   "source": [
    "### 1.3. Using the helper function, we select a repo that we want to download the model from and a desired quantization. We initate the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50901d72-c40f-441f-8e25-f1c45ead994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the download\n",
    "repo_id = \"Qwen/Qwen2.5-7B-Instruct-GGUF\"\n",
    "download_info = client.models.initiate_model_download(repo_id, quantization='q6_k')\n",
    "\n",
    "print(f\"Downloading model: {download_info['model'].name}\")\n",
    "print(\"Files being downloaded:\")\n",
    "for file in download_info['files']:\n",
    "    print(f\"- {file.name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343fc02a-7a1d-4d3a-b54b-7157a19a6fd1",
   "metadata": {},
   "source": [
    "### 1.4. We can monitor the status of the download here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb4f86-45bf-460d-a411-5d19a5f86d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"Qwen/Qwen2.5-7B-Instruct-GGUF\"\n",
    "\n",
    "def all_downloads_complete(status):\n",
    "    return all(s.download_percentage == 100 for s in status)\n",
    "\n",
    "while True:\n",
    "    status = client.models.check_download_status(repo_id)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"Download Status for {repo_id}:\")\n",
    "    print(\"-----------------------------\")\n",
    "    \n",
    "    for s in status:\n",
    "        print(f\"Model ID: {s.m_id}\")\n",
    "        print(f\"Model File ID: {s.id}\")\n",
    "        print(f\"Model Name: {s.name}\")\n",
    "        print(f\"Download Progress: {s.download_percentage}%\")\n",
    "        print(\"-----------------------------\")\n",
    "    \n",
    "    if all_downloads_complete(status):\n",
    "        print(\"All downloads completed!\")\n",
    "        break\n",
    "    \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba6139d-76ca-4e9d-b02a-cf89ec243147",
   "metadata": {},
   "source": [
    "# 2. Now let's deploy this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1cec4c-70bc-44dd-9781-54459d422c5d",
   "metadata": {},
   "source": [
    "### 2.1. Get the default model config file (optional)\n",
    "This was created when we downloaded the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6155fea-1a0e-4059-8a1d-356577df821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model id from the status \n",
    "model_id = '847e2da0-816c-4c37-a923-5f655faa54fa'\n",
    "configs = client.models.get_model_configs(model_id)\n",
    "default_config = next((config for config in configs if config.default), configs[0])\n",
    "default_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d806e-d084-40ff-aa73-2ea3e49d9169",
   "metadata": {},
   "source": [
    "### 2.2 Deploy the model with default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd0aa9-3261-4990-85f2-acced0aea997",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_id = client.serving.deploy_model(model_id)\n",
    "deployments = client.serving.list_deployments()\n",
    "print(deployments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebbfd28-d9e5-4099-9a93-5d9996811f90",
   "metadata": {},
   "source": [
    "# 3. Let's do some inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6fbe6-6f09-401f-817d-16980c79cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "deployments = client.serving.list_deployments()\n",
    "valid_deployment = None\n",
    "for deployment in deployments:\n",
    "    if deployment.status == 'DEPLOYED' and deployment.instances:\n",
    "        valid_deployment = deployment\n",
    "        print(f\"Found a deployment of {deployment.m_name} - using it\")\n",
    "        break\n",
    "if valid_deployment is None:\n",
    "    print(\"No valid deployments found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afe098-5368-46f7-a30f-599f15b890fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_deployment.lb_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c6cae-35d7-4b3c-b80e-aaaff67c291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "http_client = httpx.Client(base_url=f\"http://localhost:{valid_deployment.lb_port}/v1\")\n",
    "\n",
    "# Initialize the OpenAI client with the custom http_client\n",
    "openai_client = OpenAI(\n",
    "    api_key=\"local\", \n",
    "    base_url=f\"http://localhost:{valid_deployment.lb_port}/v1\",\n",
    "    http_client=http_client\n",
    ")\n",
    "\n",
    "print(f\"Endpoint: {openai_client.base_url}\")\n",
    "\n",
    "# Initial sleep to let server initialize\n",
    "time.sleep(1)\n",
    "\n",
    "# Try up to 3 times with 5 second delay\n",
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        chat_completion = openai_client.chat.completions.create(\n",
    "            model=\"local-model\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"What is the capital of New Jersey? What about California? And how do I say 'green tea' in mandarin?\"}\n",
    "            ]\n",
    "        )\n",
    "        print(\"Model response:\")\n",
    "        print(chat_completion.choices[0].message.content)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        if \"404\" in str(e) and attempt < max_retries - 1:\n",
    "            print(f\"Server not ready (attempt {attempt + 1}/{max_retries}), waiting 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        print(f\"An error occurred during inference: {e}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7934fa53-205d-4d87-aeb8-7b53c9d9a687",
   "metadata": {},
   "source": [
    "# Quick Model Download and Deployment with Kamiwaza SDK\n",
    "\n",
    "This notebook demonstrates the simplified approach to download and deploy models using Kamiwaza SDK's all-in-one function. While the previous notebook showed the step-by-step process, Kamiwaza has packaged everything into a single convenient function to streamline the workflow.\n",
    "\n",
    "The `download_and_deploy_model` function handles:\n",
    "1. Finding the model\n",
    "2. Downloading the appropriate files\n",
    "3. Waiting for download completion\n",
    "4. Deploying the model\n",
    "5. Setting up the endpoint\n",
    "\n",
    "All with just one line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58456fae-f0c1-4579-a800-9331882754a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kamiwaza_client import KamiwazaClient\n",
    "client = KamiwazaClient(\"http://localhost:7777/api/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84223c22-0bfc-4d53-841f-3385cde230c6",
   "metadata": {},
   "source": [
    "## Search for a Model\n",
    "\n",
    "Before downloading, we can search for the model to view available quantization options. This step is optional but helpful to see what's available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9d3bd6a-e78d-4eca-8b15-5548f3b1ae55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model: Llama-3-8B-Instruct-Coder-v2-GGUF\n",
       " Repo ID: bartowski/Llama-3-8B-Instruct-Coder-v2-GGUF\n",
       " Files: 26 available\n",
       "   GITATTRIBUTES files: 1\n",
       "   GGUF files: 23\n",
       "   IMATRIX files: 1\n",
       "   MD files: 1\n",
       " Available quantizations:\n",
       "   - fp16\n",
       "   - iq1_m\n",
       "   - iq1_s\n",
       "   - iq2_m\n",
       "   - iq2_s\n",
       "   - iq2_xs\n",
       "   - iq2_xxs\n",
       "   - iq3_m\n",
       "   - iq3_s\n",
       "   - iq3_xs\n",
       "   - iq3_xxs\n",
       "   - iq4_nl\n",
       "   - iq4_xs\n",
       "   - q2_k\n",
       "   - q3_k\n",
       "   - q4_k\n",
       "   - q5_k\n",
       "   - q6_k\n",
       "   - q8_0\n",
       " Files: 0 downloading]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_repo='bartowski/Llama-3-8B-Instruct-Coder-v2-GGUF'\n",
    "client.models.search_models(hf_repo, exact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46794d7d-1eaf-4836-93fc-0985c1c22d85",
   "metadata": {},
   "source": [
    "## Download and Deploy in One Step\n",
    "\n",
    "Now for the simplified approach - we can download and deploy the model in a single function call. This function:\n",
    "\n",
    "- Initiates the download with the specified quantization\n",
    "- Monitors download progress with real-time updates\n",
    "- Automatically deploys the model once download is complete\n",
    "- Returns complete information about the model and deployment\n",
    "\n",
    "You can specify any quantization level shown in the search results, or omit the parameter to use the default best option for your hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14922caf-303b-4c5d-9f60-c23d859dc2c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating download for bartowski/Llama-3-8B-Instruct-Coder-v2-GGUF with quantization q6_k...\n",
      "Model files for bartowski/Llama-3-8B-Instruct-Coder-v2-GGUF are already downloaded.\n",
      "Deploying model bartowski/Llama-3-8B-Instruct-Coder-v2-GGUF...\n",
      "Model bartowski/Llama-3-8B-Instruct-Coder-v2-GGUF successfully deployed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': Model: Llama-3-8B-Instruct-Coder-v2-GGUF\n",
       " Repo ID: bartowski/Llama-3-8B-Instruct-Coder-v2-GGUF\n",
       " ID: d4d8e9ef-c465-4213-b870-b1e193b74c0c\n",
       " Created: 2025-03-06 19:31:12.933542\n",
       " Files: 26 available\n",
       "   GGUF files: 23\n",
       "   GITATTRIBUTES files: 1\n",
       "   IMATRIX files: 1\n",
       "   MD files: 1\n",
       " Files: 0 downloading,\n",
       " 'target_files': [ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ2_S.gguf\n",
       "  ID: 06a67c8f-23ee-4277-a59b-a3b96d17a800\n",
       "  Size: 2.57 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-Q4_K_M.gguf\n",
       "  ID: 258665e7-8530-478c-a77f-a3a420037118\n",
       "  Size: 4.58 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-fp16.gguf\n",
       "  ID: 25f19f25-4569-4fce-8c81-48cd89e96254\n",
       "  Size: 14.97 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-Q5_K_M.gguf\n",
       "  ID: 26417de8-92f9-445b-8fcb-689fd993ed2d\n",
       "  Size: 5.34 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ2_XS.gguf\n",
       "  ID: 27ce0dd6-8773-4a05-8376-b4b08c740d65\n",
       "  Size: 2.43 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ3_S.gguf\n",
       "  ID: 3138718d-e64c-4ed3-b4a4-d243032daaa3\n",
       "  Size: 3.43 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ4_NL.gguf\n",
       "  ID: 36318898-7bfb-4e95-af6d-7bb343cdeff1\n",
       "  Size: 4.36 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-Q3_K_S.gguf\n",
       "  ID: 3744ca69-b6dd-406a-beb7-3e8d3d3bb573\n",
       "  Size: 3.41 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ3_M.gguf\n",
       "  ID: 5dafc877-713d-4fce-a957-24b5fb4cd451\n",
       "  Size: 3.52 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ2_XXS.gguf\n",
       "  ID: 71fbf0c3-64b2-4dd0-aacd-a375edfeb385\n",
       "  Size: 2.23 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-Q2_K.gguf\n",
       "  ID: 766395f0-1fdb-4822-be96-70bbee138e6c\n",
       "  Size: 2.96 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-Q6_K.gguf\n",
       "  ID: 7e3efc3e-ac3f-480e-a5b1-2d81685a8f87\n",
       "  Size: 6.14 GB\n",
       "  Storage Type: file,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ2_M.gguf\n",
       "  ID: 7f6bbf76-95df-43b9-b598-c939823c783f\n",
       "  Size: 2.75 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ3_XS.gguf\n",
       "  ID: 98ef942e-2e59-4ec8-aa38-a86370d2f1ac\n",
       "  Size: 3.28 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-Q4_K_S.gguf\n",
       "  ID: bc5b3047-4811-4dee-894d-0a8bcd71ae80\n",
       "  Size: 4.37 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ1_S.gguf\n",
       "  ID: c43e5522-44fc-40d8-87b1-3855d95d2cff\n",
       "  Size: 1.88 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ4_XS.gguf\n",
       "  ID: c49c4754-bc97-40ee-9f26-c073f0894ccc\n",
       "  Size: 4.14 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-Q3_K_M.gguf\n",
       "  ID: d3feae69-4ade-4e51-8d8e-9c311ef2a2a7\n",
       "  Size: 3.74 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-Q5_K_S.gguf\n",
       "  ID: eb2c3460-8613-4649-9457-91d7e9b2fcd7\n",
       "  Size: 5.21 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ1_M.gguf\n",
       "  ID: f1e0da3f-173b-4424-9be3-d6ef3ffa9fcc\n",
       "  Size: 2.01 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-IQ3_XXS.gguf\n",
       "  ID: f44b4106-dd85-449e-93a9-cbe8aa6ef9dd\n",
       "  Size: 3.05 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-Q8_0.gguf\n",
       "  ID: f6059d91-1e7d-44be-9274-4334bca2f91b\n",
       "  Size: 7.95 GB,\n",
       "  ModelFile: Llama-3-8B-Instruct-Coder-v2-Q3_K_L.gguf\n",
       "  ID: f7ce27df-7922-4976-9fec-47e9c8f44093\n",
       "  Size: 4.03 GB],\n",
       " 'downloading_files': [],\n",
       " 'downloaded_files': [ModelFile: Llama-3-8B-Instruct-Coder-v2-Q6_K.gguf\n",
       "  ID: 7e3efc3e-ac3f-480e-a5b1-2d81685a8f87\n",
       "  Size: 6.14 GB\n",
       "  Storage Type: file],\n",
       " 'pending_files': [],\n",
       " 'total_progress': 100,\n",
       " 'all_downloaded': True,\n",
       " 'any_downloading': False,\n",
       " 'deployment_id': UUID('07dd83f4-83ed-44f1-856c-05918f96ea24')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.download_and_deploy_model(hf_repo, quantization = 'q6_k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f5d32-cdb4-4a88-843b-33fbff2cd042",
   "metadata": {},
   "source": [
    "## Set Up the OpenAI Client\n",
    "\n",
    "Once the model is deployed, we can get an OpenAI-compatible client to interact with it, just like in the step-by-step approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ac1233f-4b01-443b-b23a-e4924e0eb437",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = client.openai.get_client(repo_id=hf_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6b61bf9-870f-4c79-a125-542219d1ed02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-07 13:55:29,195 - httpx - INFO - HTTP Request: POST http://localhost:51123/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3"
     ]
    }
   ],
   "source": [
    "# Create a streaming chat completion\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"How many r's are in the word 'strawberry'? ONLY RESPOND WITH A SINGLE NUMBER\"}\n",
    "    ],\n",
    "    model=\"model\",\n",
    "    stream=True \n",
    ")\n",
    "\n",
    "# display the stream\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb5c95-d1b1-4a0f-9301-0e38baa62bfc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `download_and_deploy_model` function provides a streamlined way to go from finding a model to using it for inference in minimal steps. This is especially useful for:\n",
    "\n",
    "- Quick experimentation with different models\n",
    "- Simplified deployment workflows\n",
    "- Reducing boilerplate code in applications\n",
    "\n",
    "When you're done, you can still stop the deployment using `client.serving.stop_deployment(repo_id=hf_repo)` as shown in the first notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

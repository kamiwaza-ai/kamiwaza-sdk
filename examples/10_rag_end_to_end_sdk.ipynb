{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End RAG with Kamiwaza SDK (Offsets-Only)\n",
    "\n",
    "This notebook demonstrates a complete RAG flow using the Kamiwaza SDK without storing chunk text in the vector database. The flow:\n",
    "\n",
    "1. Discover Markdown documents in `notebooks/sdk/`\n",
    "2. Register them in the catalog\n",
    "3. Chunk with precise byte offsets and embed\n",
    "4. Insert vectors with offsets-only metadata into Milvus\n",
    "5. Enter a query, retrieve top-K relevant chunks by vector search\n",
    "6. Reconstruct chunk text from source files via offsets and generate an answer using the first active deployed model\n",
    "\n",
    "Notes:\n",
    "- You’ll need the Kamiwaza API running locally and a deployment available for the LLM step.\n",
    "- Offsets-only storage keeps Milvus lean and uses the source files as the single source of truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API: http://localhost:7777/api/\n",
      "Docs dir: /Users/tylerhouchin/code/kamiwaza/notebooks/sdk-examples\n",
      "Collection: SDKRAG_1754676135\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "from kamiwaza_client import KamiwazaClient\n",
    "\n",
    "API_URL = os.environ.get(\"KAMIWAZA_API_URL\", \"http://localhost:7777/api/\")\n",
    "DOCS_DIR = Path.cwd()  # assume MD files are alongside the notebook\n",
    "\n",
    "EMBEDDER_MODEL = \"BAAI/bge-base-en-v1.5\"\n",
    "PROVIDER_TYPE = \"huggingface_embedding\"\n",
    "CHUNK_SIZE = 600\n",
    "OVERLAP = 102\n",
    "TOP_K = 5\n",
    "COLLECTION_NAME = f\"SDKRAG_{int(time.time())}\"\n",
    "\n",
    "client = KamiwazaClient(API_URL)\n",
    "print(f\"API: {API_URL}\")\n",
    "print(f\"Docs dir: {DOCS_DIR}\")\n",
    "print(f\"Collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def list_markdown_files(root: Path, max_files: int = 1000) -> List[Path]:\n",
    "    files: List[Path] = []\n",
    "    if root.is_file():\n",
    "        if root.suffix.lower() == \".md\":\n",
    "            return [root]\n",
    "        return []\n",
    "    for p in root.rglob(\"*.md\"):\n",
    "        if \".ipynb_checkpoints\" in p.parts:\n",
    "            continue\n",
    "        files.append(p)\n",
    "        if len(files) >= max_files:\n",
    "            break\n",
    "    return files\n",
    "\n",
    "\n",
    "def safe_read_utf8_window(path: Path, start: int, length: int) -> str:\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            f.seek(max(0, start))\n",
    "            data = f.read(max(0, length))\n",
    "        try:\n",
    "            return data.decode(\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            return data.decode(\"utf-8\", errors=\"ignore\")\n",
    "    except Exception as exc:\n",
    "        return f\"<error reading bytes: {exc}>\"\n",
    "\n",
    "\n",
    "def rechunk_get_chunk_text(\n",
    "    client: KamiwazaClient,\n",
    "    model: str,\n",
    "    provider_type: str,\n",
    "    source_path: Path,\n",
    "    offset: int,\n",
    "    chunk_size: int,\n",
    "    overlap: int,\n",
    ") -> Optional[str]:\n",
    "    try:\n",
    "        text = source_path.read_text(encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    embedder = client.embedding.get_embedder(\n",
    "        model=model,\n",
    "        provider_type=provider_type,\n",
    "    )\n",
    "    resp = embedder.chunk_text(\n",
    "        text=text,\n",
    "        max_length=chunk_size,\n",
    "        overlap=overlap,\n",
    "        return_metadata=True,\n",
    "    )\n",
    "\n",
    "    chunks = resp.chunks if hasattr(resp, \"chunks\") else resp.get(\"chunks\", [])\n",
    "    offsets = resp.offsets if hasattr(resp, \"offsets\") else resp.get(\"offsets\", [])\n",
    "\n",
    "    for ch_text, ch_off in zip(chunks, offsets or []):\n",
    "        if int(ch_off) == int(offset):\n",
    "            return ch_text\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 docs\n",
      "Cataloged: kz_tech_info.md -> urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk-examples/kz_tech_info.md,PROD)\n",
      "Cataloged: kz_marketing.md -> urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk-examples/kz_marketing.md,PROD)\n"
     ]
    }
   ],
   "source": [
    "# 1) Discover & Catalog\n",
    "files = list_markdown_files(DOCS_DIR)\n",
    "if not files:\n",
    "    raise ValueError(f\"No markdown files found in {DOCS_DIR}\")\n",
    "\n",
    "print(f\"Found {len(files)} docs\")\n",
    "\n",
    "path_to_urn: Dict[Path, str] = {}\n",
    "for fp in files:\n",
    "    ds = client.catalog.create_dataset(\n",
    "        dataset_name=str(fp.resolve()),\n",
    "        platform=\"file\",\n",
    "        environment=\"PROD\",\n",
    "        description=f\"RAG demo: {fp.name}\",\n",
    "    )\n",
    "    path_to_urn[fp] = ds.urn\n",
    "    print(f\"Cataloged: {fp.name} -> {ds.urn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: SDKRAG_1754676135\n",
      "\n",
      "Processing: kz_tech_info.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 14:02:18,814 - kamiwaza_client.services.embedding - INFO - Starting embedding generation for 7 chunks (batch size: 64)\n",
      "2025-08-08 14:02:18,935 - kamiwaza_client.services.embedding - INFO - Successfully generated embeddings for 7 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - chunks: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 14:02:20,280 - kamiwaza_client.services.embedding - INFO - Starting embedding generation for 6 chunks (batch size: 64)\n",
      "2025-08-08 14:02:20,396 - kamiwaza_client.services.embedding - INFO - Successfully generated embeddings for 6 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - inserted: 7\n",
      "\n",
      "Processing: kz_marketing.md\n",
      " - chunks: 6\n",
      " - inserted: 6\n",
      "\n",
      "Inserted total: 13\n"
     ]
    }
   ],
   "source": [
    "# 2) Chunk, Embed, and Insert (Offsets-Only)\n",
    "embedder = client.embedding.get_embedder(model=EMBEDDER_MODEL, provider_type=PROVIDER_TYPE)\n",
    "\n",
    "inserted_total = 0\n",
    "print(f\"Collection: {COLLECTION_NAME}\")\n",
    "\n",
    "for fp in files:\n",
    "    print(f\"\\nProcessing: {fp.name}\")\n",
    "    text = fp.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # chunk with offsets\n",
    "    resp = embedder.chunk_text(text=text, max_length=CHUNK_SIZE, overlap=OVERLAP, return_metadata=True)\n",
    "    chunks = resp.chunks if hasattr(resp, \"chunks\") else resp[\"chunks\"]\n",
    "    offsets = resp.offsets if hasattr(resp, \"offsets\") else resp.get(\"offsets\", [])\n",
    "    print(f\" - chunks: {len(chunks)}\")\n",
    "\n",
    "    # embed\n",
    "    vectors = embedder.embed_chunks(chunks)\n",
    "\n",
    "    # metadata (autofields only)\n",
    "    metadata = []\n",
    "    for off in (offsets or [0]*len(chunks)):\n",
    "        metadata.append({\n",
    "            \"model_name\": EMBEDDER_MODEL,\n",
    "            \"source\": str(fp.resolve()),\n",
    "            \"catalog_urn\": path_to_urn[fp],\n",
    "            \"offset\": int(off),\n",
    "            \"filename\": fp.name,\n",
    "        })\n",
    "\n",
    "    # insert\n",
    "    client.vectordb.insert(\n",
    "        vectors=vectors,\n",
    "        metadata=metadata,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        field_list=None,\n",
    "    )\n",
    "\n",
    "    print(f\" - inserted: {len(chunks)}\")\n",
    "    inserted_total += len(chunks)\n",
    "\n",
    "print(f\"\\nInserted total: {inserted_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Interactive Query → Retrieve → Preview\n",
    "\n",
    "def retrieve(query: str, k: int = TOP_K):\n",
    "    q_vec = embedder.create_embedding(query).embedding\n",
    "    hits = client.vectordb.search(\n",
    "        query_vector=q_vec,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        limit=k,\n",
    "        output_fields=[\"source\", \"offset\", \"filename\", \"catalog_urn\", \"model_name\"],\n",
    "    )\n",
    "    results = []\n",
    "    for h in hits:\n",
    "        score = getattr(h, \"score\", None)\n",
    "        if score is None and isinstance(h, dict):\n",
    "            score = h.get(\"score\", 0.0)\n",
    "        meta = getattr(h, \"metadata\", None) or (h.get(\"metadata\") if isinstance(h, dict) else {})\n",
    "        results.append({\"score\": float(score or 0.0), \"metadata\": meta})\n",
    "    return results\n",
    "\n",
    "\n",
    "def preview_hits(hits, mode=\"rechunk\", pre_bytes=500, post_bytes=2000):\n",
    "    print(f\"Found {len(hits)} results\\n\")\n",
    "    for i, item in enumerate(hits, 1):\n",
    "        meta = item[\"metadata\"]\n",
    "        source = meta.get(\"source\")\n",
    "        offset = int(meta.get(\"offset\", 0))\n",
    "        fname = meta.get(\"filename\")\n",
    "        score = item.get(\"score\", 0.0)\n",
    "        print(f\"{i:02d}. score={score:.4f} file={fname} offset={offset}\")\n",
    "        if not source or not Path(source).exists():\n",
    "            print(\"   <missing source>\")\n",
    "            continue\n",
    "        if mode == \"rechunk\":\n",
    "            text = rechunk_get_chunk_text(client, EMBEDDER_MODEL, PROVIDER_TYPE, Path(source), offset, CHUNK_SIZE, OVERLAP)\n",
    "            if text is None:\n",
    "                text = safe_read_utf8_window(Path(source), max(0, offset - pre_bytes), pre_bytes + post_bytes)\n",
    "        elif mode == \"around\":\n",
    "            text = safe_read_utf8_window(Path(source), max(0, offset - pre_bytes), pre_bytes + post_bytes)\n",
    "        else:  # from\n",
    "            text = safe_read_utf8_window(Path(source), offset, post_bytes)\n",
    "        print(\"--- preview ---\")\n",
    "        print(text)\n",
    "        print()\n",
    "\n",
    "# Example (you can re-run this cell):\n",
    "# hits = retrieve(\"How does Kamiwaza benefit the enterprise?\", k=TOP_K)\n",
    "# preview_hits(hits, mode=\"rechunk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Generate with the first active deployment\n",
    "\n",
    "def pick_first_active_deployment_name(client: KamiwazaClient) -> Optional[str]:\n",
    "    try:\n",
    "        deps = client.serving.list_active_deployments()\n",
    "    except Exception:\n",
    "        deps = []\n",
    "    if not deps:\n",
    "        return None\n",
    "    dep = deps[0]\n",
    "    return getattr(dep, \"m_name\", None) or getattr(dep, \"name\", None)\n",
    "\n",
    "\n",
    "def call_llm_with_context(question: str, context: str) -> str:\n",
    "    model_name = pick_first_active_deployment_name(client)\n",
    "    if not model_name:\n",
    "        return \"<No active deployments found>\\n\" + context\n",
    "    openai_client = client.openai.get_client(model_name)\n",
    "    system_prompt = (\n",
    "        \"You are a helpful assistant. Answer using ONLY the provided context. \"\n",
    "        \"If not in context, say you do not know.\"\n",
    "    )\n",
    "    user_content = f\"Question:\\n{question}\\n\\nContext:\\n{context}\"\n",
    "    try:\n",
    "        resp = openai_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ],\n",
    "            timeout=600,\n",
    "        )\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception as exc:\n",
    "        return f\"<LLM error: {exc}>\\n\" + context\n",
    "\n",
    "# Example workflow\n",
    "# query = \"How does Kamiwaza benefit the enterprise?\"\n",
    "# hits = retrieve(query, k=TOP_K)\n",
    "# preview_hits(hits)\n",
    "# ctx, cits = build_context = (lambda *args, **kwargs: None), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 14:02:49,520 - httpx - INFO - HTTP Request: POST http://localhost:61101/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== RAG ANSWER =====\n",
      "\n",
      "Kamiwaza benefits the enterprise by addressing critical challenges in AI deployment and management, offering the following key advantages:  \n",
      "\n",
      "1. **Cost Optimization**:  \n",
      "   - Reduces AI infrastructure costs by up to **70%** through intelligent GPU resource allocation and multi-model serving on single GPUs.  \n",
      "   - Eliminates per-request pricing models, lowering expenses for high-volume workloads.  \n",
      "\n",
      "2. **Enterprise-Grade Security & Compliance**:  \n",
      "   - Built-in security features like role-based access control, audit logging, and support for authentication methods (JWT, SAML, Auth0).  \n",
      "   - Air-gapped deployment options and compliance certifications for regulated industries.  \n",
      "\n",
      "3. **Scalability & Flexibility**:  \n",
      "   - Supports hybrid, on-premise, and cloud deployments, with automatic scaling and load balancing.  \n",
      "   - Universal compatibility with all major AI frameworks (VLLM, LlamaCpp, MLX) and hardware (NVIDIA, AMD, Intel GPUs, Apple Silicon).  \n",
      "\n",
      "4. **Accelerated AI Adoption**:  \n",
      "   - Enables deployment of AI models in **minutes** via one-click processes, reducing the time-to-production from months.  \n",
      "   - Pre-built integrations with HuggingFace and model hubs, allowing teams to leverage thousands of pre-trained models.  \n",
      "\n",
      "5. **Operational Efficiency**:  \n",
      "   - Simplifies infrastructure management, letting data scientists focus on innovation rather than server maintenance.  \n",
      "   - Intelligent resource management maximizes GPU utilization and reduces idle resource costs.  \n",
      "\n",
      "6. **Competitive Differentiators**:  \n",
      "   - Avoids vendor lock-in by supporting deployment anywhere (cloud, on-premise, hybrid).  \n",
      "   - Provides production-ready capabilities, enterprise support, and integrated security, unlike DIY open-source solutions.  \n",
      "\n",
      "These benefits empower enterprises to innovate faster, reduce risks, and achieve ROI through streamlined AI workflows.\n",
      "\n",
      "======================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5) Run an example query end-to-end (adjust the query and re-run)\n",
    "query = \"How does Kamiwaza benefit the enterprise?\"\n",
    "\n",
    "hits = retrieve(query, k=TOP_K)\n",
    "#preview_hits(hits, mode=\"rechunk\")\n",
    "\n",
    "# Build context for the LLM\n",
    "# Reuse the same logic as previews; cap to ~8k chars\n",
    "ctx_parts = []\n",
    "for i, item in enumerate(hits, 1):\n",
    "    meta = item[\"metadata\"]\n",
    "    src = meta.get(\"source\")\n",
    "    off = int(meta.get(\"offset\", 0))\n",
    "    if not src or not Path(src).exists():\n",
    "        continue\n",
    "    text = rechunk_get_chunk_text(client, EMBEDDER_MODEL, PROVIDER_TYPE, Path(src), off, CHUNK_SIZE, OVERLAP)\n",
    "    if text is None:\n",
    "        text = safe_read_utf8_window(Path(src), max(0, off - 500), 2500)\n",
    "    header = f\"[Source {i}] {Path(src).name} @ offset {off}\"\n",
    "    ctx_parts.append(f\"{header}\\n---\\n{text}\\n\")\n",
    "\n",
    "context = \"\\n\\n\".join(ctx_parts)[:8000]\n",
    "\n",
    "answer = call_llm_with_context(query, context)\n",
    "print(\"\\n===== RAG ANSWER =====\\n\")\n",
    "print(answer)\n",
    "print(\"\\n======================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

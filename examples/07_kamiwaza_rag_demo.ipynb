{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kamiwaza RAG Demo: Document Processing with Catalog Integration\n",
    "\n",
    "This notebook demonstrates the complete RAG (Retrieval-Augmented Generation) workflow using Kamiwaza SDK:\n",
    "1. Upload documents to the catalog\n",
    "2. Chunk documents into manageable pieces\n",
    "3. Generate embeddings for each chunk\n",
    "4. Store vectors with chunk text as metadata \n",
    "5. Perform semantic search on the content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# Import Kamiwaza SDK\n",
    "from kamiwaza_sdk import KamiwazaClient as kz\n",
    "from kamiwaza_sdk.schemas.catalog import Dataset\n",
    "from kamiwaza_sdk.schemas.vectordb import (\n",
    "    InsertVectorsRequest,\n",
    "    SearchVectorsRequest,\n",
    "    SearchResult\n",
    ")\n",
    "\n",
    "# Initialize the Kamiwaza client\n",
    "KAMIWAZA_API_URL = \"http://localhost:7777/api/\"\n",
    "client = kz(base_url=KAMIWAZA_API_URL)\n",
    "\n",
    "print(f\"Connected to Kamiwaza at: {KAMIWAZA_API_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Upload to Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to add files to catalog (based on the example)\n",
    "def add_files_to_catalog(filepaths, client, platform=\"file\", recursive=True, description=\"\"):\n",
    "    \"\"\"\n",
    "    Add files to the Kamiwaza catalog.\n",
    "    \n",
    "    Args:\n",
    "        filepaths: List of file paths or a single file path\n",
    "        client: kz instance\n",
    "        platform: Platform identifier (default: \"file\")\n",
    "        recursive: Whether to process directories recursively\n",
    "        description: Description for the datasets\n",
    "    \n",
    "    Returns:\n",
    "        List of URNs for created datasets\n",
    "    \"\"\"\n",
    "    if isinstance(filepaths, str):\n",
    "        filepaths = [filepaths]\n",
    "    \n",
    "    urns = []\n",
    "    datasets = []  # Keep track of dataset objects\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        try:\n",
    "            # Create dataset for each file\n",
    "            dataset = client.catalog.create_dataset(\n",
    "                dataset_name=filepath,\n",
    "                platform=platform,\n",
    "                environment=\"PROD\",\n",
    "                description=description or f\"File: {Path(filepath).name}\"\n",
    "            )\n",
    "            \n",
    "            if dataset.urn:\n",
    "                urns.append(dataset.urn)\n",
    "                datasets.append(dataset)  # Store the dataset object\n",
    "                print(f\"‚úÖ Added to catalog: {Path(filepath).name}\")\n",
    "                print(f\"   URN: {dataset.urn}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error adding {filepath}: {str(e)}\")\n",
    "    \n",
    "    # Return both URNs and datasets\n",
    "    return urns, datasets\n",
    "\n",
    "# Function to show dataset info\n",
    "def show_dataset_info(client, urns):\n",
    "    \"\"\"Display information about datasets in the catalog.\"\"\"\n",
    "    all_datasets = client.catalog.list_datasets()\n",
    "    my_datasets = [d for d in all_datasets if d.urn in urns]\n",
    "    \n",
    "    print(f\"Total datasets in catalog: {len(all_datasets)}\")\n",
    "    print(f\"Matching datasets: {len(my_datasets)}\")\n",
    "    \n",
    "    for d in my_datasets:\n",
    "        print(f\"\\nURN: {d.urn}\")\n",
    "        print(f\"ID: {d.id}\")\n",
    "        print(f\"Platform: {d.platform}\")\n",
    "        print(f\"Environment: {d.environment}\")\n",
    "        print(f\"Name: {d.name if d.name else 'None'}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return my_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Upload documents to catalog\n",
    "# Replace with your document paths\n",
    "DOCUMENT_PATHS = [\n",
    "    \"./kz_marketing.md\",  \n",
    "    \"./kz_tech_info.md\"\n",
    "    # Add more documents as needed\n",
    "]\n",
    "\n",
    "# Add files to catalog - now returns both URNs and datasets\n",
    "urns, datasets_created = add_files_to_catalog(\n",
    "    filepaths=DOCUMENT_PATHS,\n",
    "    client=client,\n",
    "    platform=\"file\",\n",
    "    description=\"RAG documents\"\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(urns)} datasets in catalog\")\n",
    "\n",
    "# Show dataset information from list_datasets\n",
    "if urns:\n",
    "    print(\"\\nDataset Information from Catalog:\")\n",
    "    datasets_from_catalog = show_dataset_info(client, urns)\n",
    "    \n",
    "    # Use the datasets we created directly if list_datasets doesn't return them\n",
    "    if not datasets_from_catalog and datasets_created:\n",
    "        print(\"\\nUsing created datasets directly:\")\n",
    "        datasets = datasets_created\n",
    "        for d in datasets:\n",
    "            print(f\"\\nURN: {d.urn}\")\n",
    "            print(f\"ID: {d.id}\")\n",
    "            print(f\"Platform: {d.platform}\")\n",
    "            print(f\"Environment: {d.environment}\")\n",
    "            print(f\"Name: {d.name if d.name else 'None'}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        datasets = datasets_from_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Chunking and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for chunking and embedding\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 102  # Token overlap between chunks\n",
    "EMBEDDER_MODEL = \"BAAI/bge-base-en-v1.5\" \n",
    "\n",
    "# Use a unique collection name to avoid schema conflicts\n",
    "import time\n",
    "timestamp = int(time.time())\n",
    "COLLECTION_NAME = f\"TestRAG_{timestamp}\"\n",
    "\n",
    "print(f\"Chunking configuration:\")\n",
    "print(f\"  - Chunk size: {CHUNK_SIZE} tokens\")\n",
    "print(f\"  - Overlap: {CHUNK_OVERLAP} tokens\")\n",
    "print(f\"  - Embedding model: {EMBEDDER_MODEL}\")\n",
    "print(f\"  - Collection name: {COLLECTION_NAME}\")\n",
    "print(f\"\\nNote: Using a new collection to ensure clean schema\")\n",
    "\n",
    "# Ensure a fresh collection schema if a name collision occurs\n",
    "existing_collections = client.vectordb.list_collections()\n",
    "if COLLECTION_NAME in existing_collections:\n",
    "    print(f\"\\nDropping existing collection '{COLLECTION_NAME}' to ensure clean schema\")\n",
    "    client.vectordb.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "# Create a global embedder instance to prevent cleanup between operations\n",
    "print(f\"\\nInitializing global embedder...\")\n",
    "GLOBAL_EMBEDDER = client.embedding.get_embedder(\n",
    "    model=EMBEDDER_MODEL,\n",
    "    provider_type=\"huggingface_embedding\"\n",
    ")\n",
    "print(f\"‚úÖ Global embedder initialized and ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_utf8_bytes(text: str, max_bytes: int = 1000, suffix: str = \"...\") -> str:\n",
    "    \"\"\"Truncate a string so its UTF-8 byte length <= max_bytes (including suffix).\"\"\"\n",
    "    raw = text.encode(\"utf-8\")\n",
    "    if len(raw) <= max_bytes:\n",
    "        return text\n",
    "    suffix_bytes = suffix.encode(\"utf-8\")\n",
    "    limit = max(0, max_bytes - len(suffix_bytes))\n",
    "    candidate = raw[:limit]\n",
    "    # Ensure we don't cut a multi-byte character in the middle\n",
    "    while True:\n",
    "        try:\n",
    "            head = candidate.decode(\"utf-8\")\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            candidate = candidate[:-1]\n",
    "            if not candidate:\n",
    "                head = \"\"\n",
    "                break\n",
    "    return head + suffix\n",
    "\n",
    "\n",
    "def process_document_from_catalog(dataset: Dataset, collection_name: str, embedder=None):\n",
    "    \"\"\"\n",
    "    Process a document from the catalog: chunk, embed, and store in vector DB.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The catalog dataset containing the document\n",
    "        collection_name: Name of the vector collection to store chunks\n",
    "        embedder: Optional embedder instance (uses GLOBAL_EMBEDDER if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        Number of chunks processed\n",
    "    \"\"\"\n",
    "    # Use provided embedder or global instance\n",
    "    if embedder is None:\n",
    "        embedder = GLOBAL_EMBEDDER\n",
    "    \n",
    "    # Extract file path from dataset\n",
    "    # The dataset id contains the file path\n",
    "    doc_path = Path(dataset.id)\n",
    "    \n",
    "    # Fallback: Parse path from URN if needed\n",
    "    if not doc_path.exists() and dataset.urn:\n",
    "        # URN format: urn:li:dataset:(urn:li:dataPlatform:file,/path/to/file,PROD)\n",
    "        parts = dataset.urn.split(',')\n",
    "        if len(parts) >= 2:\n",
    "            doc_path = Path(parts[1])\n",
    "    \n",
    "    if not doc_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {doc_path}\")\n",
    "    \n",
    "    # Read document content\n",
    "    with open(doc_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    print(f\"üìÑ Processing document: {doc_path.name}\")\n",
    "    print(f\"   - Size: {len(content)} characters\")\n",
    "    print(f\"   - URN: {dataset.urn}\")\n",
    "    \n",
    "    try:\n",
    "        # Chunk the document\n",
    "        print(f\"\\nüìã Chunking document...\")\n",
    "        chunks = embedder.chunk_text(\n",
    "            text=content,\n",
    "            max_length=CHUNK_SIZE,\n",
    "            overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "        \n",
    "        print(f\"   - Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Generate embeddings for all chunks\n",
    "        print(f\"\\nüßÆ Generating embeddings...\")\n",
    "        embeddings = embedder.embed_chunks(chunks)\n",
    "        \n",
    "        # Prepare metadata for each chunk\n",
    "        # Include both autofields and custom fields\n",
    "        metadata_list = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Truncate chunk text to respect Milvus VARCHAR 1000-byte limit (UTF-8)\n",
    "            chunk_text = truncate_utf8_bytes(chunk, max_bytes=1000, suffix=\"...\")\n",
    "            # Debug: print UTF-8 byte length to verify\n",
    "            print(len(chunk_text.encode('utf-8')))\n",
    "            \n",
    "            metadata = {\n",
    "                # Required autofields (these MUST be included)\n",
    "                \"model_name\": EMBEDDER_MODEL,\n",
    "                \"source\": str(doc_path),\n",
    "                \"catalog_urn\": dataset.urn or \"\",\n",
    "                \"offset\": i,\n",
    "                \"filename\": doc_path.name,\n",
    "                \n",
    "                # Custom fields - these will be added to the schema\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"chunk_index\": i,\n",
    "                \"chunk_size\": len(chunk),\n",
    "            }\n",
    "            metadata_list.append(metadata)\n",
    "        \n",
    "        # Insert vectors into the database\n",
    "        print(f\"\\nüíæ Inserting vectors into collection '{collection_name}'...\")\n",
    "        \n",
    "        # Define custom fields for the collection schema\n",
    "        # IMPORTANT: Use tuple format (field_name, field_type)\n",
    "        field_list = [\n",
    "            (\"chunk_text\", \"str\"),      # Store the actual chunk text\n",
    "            (\"chunk_index\", \"int\"),     # Store chunk index\n",
    "            (\"chunk_size\", \"int\"),      # Store chunk size\n",
    "        ]\n",
    "        \n",
    "        # Use the SDK's insert method with field_list\n",
    "        response = client.vectordb.insert(\n",
    "            vectors=embeddings,\n",
    "            metadata=metadata_list,\n",
    "            collection_name=collection_name,\n",
    "            field_list=field_list  # Pass custom fields\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Successfully inserted {len(chunks)} vectors\")\n",
    "        print(f\"   - Collection: {collection_name}\")\n",
    "        print(f\"   - Custom fields added: chunk_text, chunk_index, chunk_size\")\n",
    "        print(f\"   - Chunk text stored directly in Milvus!\")\n",
    "        \n",
    "        return len(chunks)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during processing: {str(e)}\")\n",
    "        print(f\"\\nDebug info:\")\n",
    "        print(f\"  - Collection name: {collection_name}\")\n",
    "        print(f\"  - Number of vectors: {len(embeddings) if 'embeddings' in locals() else 'N/A'}\")\n",
    "        print(f\"  - Number of metadata entries: {len(metadata_list) if 'metadata_list' in locals() else 'N/A'}\")\n",
    "        print(f\"  - Field list: {field_list if 'field_list' in locals() else 'N/A'}\")\n",
    "        if 'metadata_list' in locals() and metadata_list:\n",
    "            print(f\"  - Sample metadata keys: {list(metadata_list[0].keys())}\")\n",
    "        raise\n",
    "\n",
    "# Function to process all datasets from catalog\n",
    "def process_catalog_datasets(datasets, collection_name):\n",
    "    \"\"\"Process multiple datasets from the catalog.\"\"\"\n",
    "    total_chunks = 0\n",
    "    \n",
    "    # Use the global embedder for all datasets\n",
    "    for dataset in datasets:\n",
    "        print(dataset)\n",
    "        try:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            chunks = process_document_from_catalog(dataset, collection_name, embedder=GLOBAL_EMBEDDER)\n",
    "            total_chunks += chunks\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing dataset {dataset.id}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Total chunks processed: {total_chunks}\")\n",
    "    return total_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process documents from catalog\n",
    "if datasets:\n",
    "    total_chunks = process_catalog_datasets(datasets, COLLECTION_NAME)\n",
    "else:\n",
    "    print(\"No datasets found in catalog to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, collection_name: str, limit: int = 5, embedder=None):\n",
    "    \"\"\"\n",
    "    Perform semantic search on the document collection.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        collection_name: Name of the vector collection\n",
    "        limit: Maximum number of results to return\n",
    "        embedder: Optional embedder instance (uses GLOBAL_EMBEDDER if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        Search results with chunk text and metadata\n",
    "    \"\"\"\n",
    "    # Use provided embedder or global instance\n",
    "    if embedder is None:\n",
    "        embedder = GLOBAL_EMBEDDER\n",
    "        \n",
    "    print(f\"üîç Searching for: '{query}'\")\n",
    "    print(f\"   - Collection: {collection_name}\")\n",
    "    print(f\"   - Max results: {limit}\\n\")\n",
    "    \n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedder.create_embedding(query).embedding\n",
    "    \n",
    "    # Perform vector search using the simplified API\n",
    "    # Include custom fields in output_fields\n",
    "    results = client.vectordb.search(\n",
    "        query_vector=query_embedding,\n",
    "        collection_name=collection_name,\n",
    "        limit=limit,\n",
    "        output_fields=[\"source\", \"offset\", \"filename\", \"catalog_urn\", \"model_name\", \"chunk_text\", \"chunk_index\", \"chunk_size\"]\n",
    "    )\n",
    "    \n",
    "    # The search returns a list directly, not an object with .results\n",
    "    print(f\"Found {len(results)} relevant chunks:\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        # Each result is likely a dict or object with metadata\n",
    "        # Let's check the structure\n",
    "        if hasattr(result, 'metadata'):\n",
    "            metadata = result.metadata\n",
    "        elif isinstance(result, dict) and 'metadata' in result:\n",
    "            metadata = result['metadata']\n",
    "        else:\n",
    "            metadata = {}\n",
    "            \n",
    "        # Get score\n",
    "        score = result.score if hasattr(result, 'score') else result.get('score', 0.0) if isinstance(result, dict) else 0.0\n",
    "        \n",
    "        # Get chunk text from metadata\n",
    "        chunk_text = metadata.get('chunk_text', None)\n",
    "        \n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"  üìä Score: {score:.4f}\")\n",
    "        print(f\"  üìÑ Source: {metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"  üìÅ Filename: {metadata.get('filename', 'Unknown')}\")\n",
    "        print(f\"  üìç Chunk Index: {metadata.get('chunk_index', metadata.get('offset', 'N/A'))}\")\n",
    "        print(f\"  ü§ñ Model: {metadata.get('model_name', 'N/A')}\")\n",
    "        print(f\"  üîó Catalog URN: {metadata.get('catalog_urn', 'N/A')}\")\n",
    "        print(f\"  üìè Chunk Size: {metadata.get('chunk_size', 'N/A')} chars\")\n",
    "        \n",
    "        # Display chunk text\n",
    "        if chunk_text:\n",
    "            print(f\"  üìù Content:\")\n",
    "            # Indent the chunk text\n",
    "            for line in chunk_text.split('\\n'):\n",
    "                print(f\"     {line}\")\n",
    "        else:\n",
    "            print(f\"  üìù Note: Chunk text not available in metadata\")\n",
    "            \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example searches\n",
    "queries = [\n",
    "    \"How does Kamiwaza benefit the enterprise?\",\n",
    "    \"Can Kamiwaza deploy on any hardware?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    results = semantic_search(query, COLLECTION_NAME, limit=3)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collection Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all collections\n",
    "def list_collections():\n",
    "    \"\"\"List all vector collections in the database.\"\"\"\n",
    "    collections = client.vectordb.list_collections()\n",
    "    \n",
    "    print(\"üìö Available Collections:\")\n",
    "    for collection in collections:\n",
    "        print(f\"   - {collection}\")\n",
    "    \n",
    "    return collections\n",
    "\n",
    "collections = list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean up resources\n",
    "def cleanup_collection(collection_name: str):\n",
    "    \"\"\"\n",
    "    Drop a vector collection.\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the collection to drop\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.vectordb.drop_collection(collection_name)\n",
    "        print(f\"‚úÖ Dropped collection: {collection_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error dropping collection: {str(e)}\")\n",
    "\n",
    "# Uncomment to clean up\n",
    "# cleanup_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete RAG workflow using Kamiwaza SDK:\n",
    "\n",
    "1. **Catalog Integration**: Documents are first uploaded to the catalog for centralized management\n",
    "2. **Chunking**: Documents are split into overlapping chunks for better context\n",
    "3. **Embedding**: Each chunk is converted to a vector representation\n",
    "4. **Storage**: Vectors are stored with chunk text as metadata for instant retrieval\n",
    "5. **Search**: Semantic search returns relevant chunks with their full text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kamiwaza RAG Demo: Document Processing with Catalog Integration\n",
    "\n",
    "This notebook demonstrates the complete RAG (Retrieval-Augmented Generation) workflow using Kamiwaza SDK:\n",
    "1. Upload documents to the catalog\n",
    "2. Chunk documents into manageable pieces\n",
    "3. Generate embeddings for each chunk\n",
    "4. Store vectors with chunk text as metadata\n",
    "5. Perform semantic search on the content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Kamiwaza at: http://localhost:7777/api/\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# Import Kamiwaza SDK\n",
    "from kamiwaza_client import KamiwazaClient\n",
    "from kamiwaza_client.schemas.catalog import Dataset\n",
    "from kamiwaza_client.schemas.vectordb import (\n",
    "    InsertVectorsRequest,\n",
    "    SearchVectorsRequest,\n",
    "    SearchResult\n",
    ")\n",
    "\n",
    "# Initialize the Kamiwaza client\n",
    "KAMIWAZA_API_URL = \"http://localhost:7777/api/\"\n",
    "client = KamiwazaClient(base_url=KAMIWAZA_API_URL)\n",
    "\n",
    "print(f\"Connected to Kamiwaza at: {KAMIWAZA_API_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Upload to Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to add files to catalog (based on the example)\n",
    "def add_files_to_catalog(filepaths, client, platform=\"file\", recursive=True, description=\"\"):\n",
    "    \"\"\"\n",
    "    Add files to the Kamiwaza catalog.\n",
    "    \n",
    "    Args:\n",
    "        filepaths: List of file paths or a single file path\n",
    "        client: KamiwazaClient instance\n",
    "        platform: Platform identifier (default: \"file\")\n",
    "        recursive: Whether to process directories recursively\n",
    "        description: Description for the datasets\n",
    "    \n",
    "    Returns:\n",
    "        List of URNs for created datasets\n",
    "    \"\"\"\n",
    "    if isinstance(filepaths, str):\n",
    "        filepaths = [filepaths]\n",
    "    \n",
    "    urns = []\n",
    "    datasets = []  # Keep track of dataset objects\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        try:\n",
    "            # Create dataset for each file\n",
    "            dataset = client.catalog.create_dataset(\n",
    "                dataset_name=filepath,\n",
    "                platform=platform,\n",
    "                environment=\"PROD\",\n",
    "                description=description or f\"File: {Path(filepath).name}\"\n",
    "            )\n",
    "            \n",
    "            if dataset.urn:\n",
    "                urns.append(dataset.urn)\n",
    "                datasets.append(dataset)  # Store the dataset object\n",
    "                print(f\"‚úÖ Added to catalog: {Path(filepath).name}\")\n",
    "                print(f\"   URN: {dataset.urn}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error adding {filepath}: {str(e)}\")\n",
    "    \n",
    "    # Return both URNs and datasets\n",
    "    return urns, datasets\n",
    "\n",
    "# Function to show dataset info\n",
    "def show_dataset_info(client, urns):\n",
    "    \"\"\"Display information about datasets in the catalog.\"\"\"\n",
    "    all_datasets = client.catalog.list_datasets()\n",
    "    my_datasets = [d for d in all_datasets if d.urn in urns]\n",
    "    \n",
    "    print(f\"Total datasets in catalog: {len(all_datasets)}\")\n",
    "    print(f\"Matching datasets: {len(my_datasets)}\")\n",
    "    \n",
    "    for d in my_datasets:\n",
    "        print(f\"\\nURN: {d.urn}\")\n",
    "        print(f\"ID: {d.id}\")\n",
    "        print(f\"Platform: {d.platform}\")\n",
    "        print(f\"Environment: {d.environment}\")\n",
    "        print(f\"Name: {d.name if d.name else 'None'}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return my_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added to catalog: kamiwaza.md\n",
      "   URN: urn:li:dataset:(urn:li:dataPlatform:file,/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kamiwaza.md,PROD)\n",
      "‚úÖ Added to catalog: kz_info.md\n",
      "   URN: urn:li:dataset:(urn:li:dataPlatform:file,/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md,PROD)\n",
      "\n",
      "Created 2 datasets in catalog\n",
      "\n",
      "Dataset Information from Catalog:\n",
      "Total datasets in catalog: 2\n",
      "Matching datasets: 2\n",
      "\n",
      "URN: urn:li:dataset:(urn:li:dataPlatform:file,/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kamiwaza.md,PROD)\n",
      "ID: /home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kamiwaza.md\n",
      "Platform: file\n",
      "Environment: PROD\n",
      "Name: None\n",
      "--------------------------------------------------\n",
      "\n",
      "URN: urn:li:dataset:(urn:li:dataPlatform:file,/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md,PROD)\n",
      "ID: /home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md\n",
      "Platform: file\n",
      "Environment: PROD\n",
      "Name: None\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example: Upload documents to catalog\n",
    "# Replace with your document paths\n",
    "DOCUMENT_PATHS = [\n",
    "    \"./kamiwaza.md\",  \n",
    "    \"./kz_info.md\"\n",
    "    # Add more documents as needed\n",
    "]\n",
    "\n",
    "# Add files to catalog - now returns both URNs and datasets\n",
    "urns, datasets_created = add_files_to_catalog(\n",
    "    filepaths=DOCUMENT_PATHS,\n",
    "    client=client,\n",
    "    platform=\"file\",\n",
    "    description=\"RAG documents\"\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(urns)} datasets in catalog\")\n",
    "\n",
    "# Show dataset information from list_datasets\n",
    "if urns:\n",
    "    print(\"\\nDataset Information from Catalog:\")\n",
    "    datasets_from_catalog = show_dataset_info(client, urns)\n",
    "    \n",
    "    # Use the datasets we created directly if list_datasets doesn't return them\n",
    "    if not datasets_from_catalog and datasets_created:\n",
    "        print(\"\\nUsing created datasets directly:\")\n",
    "        datasets = datasets_created\n",
    "        for d in datasets:\n",
    "            print(f\"\\nURN: {d.urn}\")\n",
    "            print(f\"ID: {d.id}\")\n",
    "            print(f\"Platform: {d.platform}\")\n",
    "            print(f\"Environment: {d.environment}\")\n",
    "            print(f\"Name: {d.name if d.name else 'None'}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        datasets = datasets_from_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Chunking and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking configuration:\n",
      "  - Chunk size: 1024 tokens\n",
      "  - Overlap: 102 tokens\n",
      "  - Embedding model: BAAI/bge-base-en-v1.5\n",
      "  - Collection name: TestRAG3\n",
      "\n",
      "Note: Using a new collection to ensure clean schema\n",
      "\n",
      "Initializing global embedder...\n",
      "‚úÖ Global embedder initialized and ready\n"
     ]
    }
   ],
   "source": [
    "# Configuration for chunking and embedding\n",
    "CHUNK_SIZE = 1024  # Maximum tokens per chunk\n",
    "CHUNK_OVERLAP = 102  # Token overlap between chunks\n",
    "EMBEDDER_MODEL = \"BAAI/bge-base-en-v1.5\"  # Use the same model as the RAG app\n",
    "\n",
    "# Use a unique collection name to avoid schema conflicts\n",
    "import time\n",
    "timestamp = int(time.time())\n",
    "COLLECTION_NAME = f\"TestRAG3\"\n",
    "\n",
    "print(f\"Chunking configuration:\")\n",
    "print(f\"  - Chunk size: {CHUNK_SIZE} tokens\")\n",
    "print(f\"  - Overlap: {CHUNK_OVERLAP} tokens\")\n",
    "print(f\"  - Embedding model: {EMBEDDER_MODEL}\")\n",
    "print(f\"  - Collection name: {COLLECTION_NAME}\")\n",
    "print(f\"\\nNote: Using a new collection to ensure clean schema\")\n",
    "\n",
    "# Create a global embedder instance to prevent cleanup between operations\n",
    "print(f\"\\nInitializing global embedder...\")\n",
    "GLOBAL_EMBEDDER = client.embedding.get_embedder(\n",
    "    model=EMBEDDER_MODEL,\n",
    "    provider_type=\"huggingface_embedding\"\n",
    ")\n",
    "print(f\"‚úÖ Global embedder initialized and ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_from_catalog(dataset: Dataset, collection_name: str, embedder=None):\n",
    "    \"\"\"\n",
    "    Process a document from the catalog: chunk, embed, and store in vector DB.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The catalog dataset containing the document\n",
    "        collection_name: Name of the vector collection to store chunks\n",
    "        embedder: Optional embedder instance (uses GLOBAL_EMBEDDER if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        Number of chunks processed\n",
    "    \"\"\"\n",
    "    # Use provided embedder or global instance\n",
    "    if embedder is None:\n",
    "        embedder = GLOBAL_EMBEDDER\n",
    "    \n",
    "    # Extract file path from dataset\n",
    "    # The dataset id contains the file path\n",
    "    doc_path = Path(dataset.id)\n",
    "    \n",
    "    # Fallback: Parse path from URN if needed\n",
    "    if not doc_path.exists() and dataset.urn:\n",
    "        # URN format: urn:li:dataset:(urn:li:dataPlatform:file,/path/to/file,PROD)\n",
    "        parts = dataset.urn.split(',')\n",
    "        if len(parts) >= 2:\n",
    "            doc_path = Path(parts[1])\n",
    "    \n",
    "    if not doc_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {doc_path}\")\n",
    "    \n",
    "    # Read document content\n",
    "    with open(doc_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    print(f\"üìÑ Processing document: {doc_path.name}\")\n",
    "    print(f\"   - Size: {len(content)} characters\")\n",
    "    print(f\"   - URN: {dataset.urn}\")\n",
    "    \n",
    "    try:\n",
    "        # Chunk the document\n",
    "        print(f\"\\nüìã Chunking document...\")\n",
    "        chunks = embedder.chunk_text(\n",
    "            text=content,\n",
    "            max_length=CHUNK_SIZE,\n",
    "            overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "        \n",
    "        print(f\"   - Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Generate embeddings for all chunks\n",
    "        print(f\"\\nüßÆ Generating embeddings...\")\n",
    "        embeddings = embedder.embed_chunks(chunks)\n",
    "        \n",
    "        # Prepare metadata for each chunk\n",
    "        # Include both autofields and custom fields\n",
    "        metadata_list = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Truncate chunk text if needed to fit VARCHAR limit (1000 chars)\n",
    "            chunk_text = chunk[:900] + \"...\" if len(chunk) > 900 else chunk\n",
    "            \n",
    "            metadata = {\n",
    "                # Required autofields (these MUST be included)\n",
    "                \"model_name\": EMBEDDER_MODEL,\n",
    "                \"source\": str(doc_path),\n",
    "                \"catalog_urn\": dataset.urn or \"\",\n",
    "                \"offset\": i,\n",
    "                \"filename\": doc_path.name,\n",
    "                \n",
    "                # Custom fields - these will be added to the schema\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"chunk_index\": i,\n",
    "                \"chunk_size\": len(chunk),\n",
    "            }\n",
    "            metadata_list.append(metadata)\n",
    "        \n",
    "        # Insert vectors into the database\n",
    "        print(f\"\\nüíæ Inserting vectors into collection '{collection_name}'...\")\n",
    "        \n",
    "        # Define custom fields for the collection schema\n",
    "        # IMPORTANT: Use tuple format (field_name, field_type)\n",
    "        field_list = [\n",
    "            (\"chunk_text\", \"str\"),      # Store the actual chunk text\n",
    "            (\"chunk_index\", \"int\"),     # Store chunk index\n",
    "            (\"chunk_size\", \"int\"),      # Store chunk size\n",
    "        ]\n",
    "        \n",
    "        # Use the SDK's insert method with field_list\n",
    "        response = client.vectordb.insert(\n",
    "            vectors=embeddings,\n",
    "            metadata=metadata_list,\n",
    "            collection_name=collection_name,\n",
    "            field_list=field_list  # Pass custom fields\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Successfully inserted {len(chunks)} vectors\")\n",
    "        print(f\"   - Collection: {collection_name}\")\n",
    "        print(f\"   - Custom fields added: chunk_text, chunk_index, chunk_size\")\n",
    "        print(f\"   - Chunk text stored directly in Milvus!\")\n",
    "        \n",
    "        return len(chunks)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during processing: {str(e)}\")\n",
    "        print(f\"\\nDebug info:\")\n",
    "        print(f\"  - Collection name: {collection_name}\")\n",
    "        print(f\"  - Number of vectors: {len(embeddings) if 'embeddings' in locals() else 'N/A'}\")\n",
    "        print(f\"  - Number of metadata entries: {len(metadata_list) if 'metadata_list' in locals() else 'N/A'}\")\n",
    "        print(f\"  - Field list: {field_list if 'field_list' in locals() else 'N/A'}\")\n",
    "        if 'metadata_list' in locals() and metadata_list:\n",
    "            print(f\"  - Sample metadata keys: {list(metadata_list[0].keys())}\")\n",
    "        raise\n",
    "\n",
    "# Function to process all datasets from catalog\n",
    "def process_catalog_datasets(datasets, collection_name):\n",
    "    \"\"\"Process multiple datasets from the catalog.\"\"\"\n",
    "    total_chunks = 0\n",
    "    \n",
    "    # Use the global embedder for all datasets\n",
    "    for dataset in datasets:\n",
    "        print(dataset)\n",
    "        try:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            chunks = process_document_from_catalog(dataset, collection_name, embedder=GLOBAL_EMBEDDER)\n",
    "            total_chunks += chunks\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing dataset {dataset.id}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Total chunks processed: {total_chunks}\")\n",
    "    return total_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urn='urn:li:dataset:(urn:li:dataPlatform:file,/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kamiwaza.md,PROD)' id='/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kamiwaza.md' platform='file' environment='PROD' paths=None name=None actor=None customProperties=None removed=None tags=None\n",
      "\n",
      "================================================================================\n",
      "üìÑ Processing document: kamiwaza.md\n",
      "   - Size: 5590 characters\n",
      "   - URN: urn:li:dataset:(urn:li:dataPlatform:file,/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kamiwaza.md,PROD)\n",
      "\n",
      "üìã Chunking document...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 14:37:03,709 - kamiwaza_client.services.embedding - INFO - Starting embedding generation for 4 chunks (batch size: 64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Created 4 chunks\n",
      "\n",
      "üßÆ Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 14:37:07,420 - kamiwaza_client.services.embedding - INFO - Successfully generated embeddings for 4 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Inserting vectors into collection 'TestRAG3'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 14:37:09,189 - kamiwaza_client.services.embedding - INFO - Starting embedding generation for 12 chunks (batch size: 64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully inserted 4 vectors\n",
      "   - Collection: TestRAG3\n",
      "   - Custom fields added: chunk_text, chunk_index, chunk_size\n",
      "   - Chunk text stored directly in Milvus!\n",
      "urn='urn:li:dataset:(urn:li:dataPlatform:file,/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md,PROD)' id='/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md' platform='file' environment='PROD' paths=None name=None actor=None customProperties=None removed=None tags=None\n",
      "\n",
      "================================================================================\n",
      "üìÑ Processing document: kz_info.md\n",
      "   - Size: 17416 characters\n",
      "   - URN: urn:li:dataset:(urn:li:dataPlatform:file,/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md,PROD)\n",
      "\n",
      "üìã Chunking document...\n",
      "   - Created 12 chunks\n",
      "\n",
      "üßÆ Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 14:37:20,808 - kamiwaza_client.services.embedding - INFO - Successfully generated embeddings for 12 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Inserting vectors into collection 'TestRAG3'...\n",
      "‚úÖ Successfully inserted 12 vectors\n",
      "   - Collection: TestRAG3\n",
      "   - Custom fields added: chunk_text, chunk_index, chunk_size\n",
      "   - Chunk text stored directly in Milvus!\n",
      "\n",
      "üéâ Total chunks processed: 16\n"
     ]
    }
   ],
   "source": [
    "# Process documents from catalog\n",
    "if datasets:\n",
    "    total_chunks = process_catalog_datasets(datasets, COLLECTION_NAME)\n",
    "else:\n",
    "    print(\"No datasets found in catalog to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, collection_name: str, limit: int = 5, embedder=None):\n",
    "    \"\"\"\n",
    "    Perform semantic search on the document collection.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        collection_name: Name of the vector collection\n",
    "        limit: Maximum number of results to return\n",
    "        embedder: Optional embedder instance (uses GLOBAL_EMBEDDER if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        Search results with chunk text and metadata\n",
    "    \"\"\"\n",
    "    # Use provided embedder or global instance\n",
    "    if embedder is None:\n",
    "        embedder = GLOBAL_EMBEDDER\n",
    "        \n",
    "    print(f\"üîç Searching for: '{query}'\")\n",
    "    print(f\"   - Collection: {collection_name}\")\n",
    "    print(f\"   - Max results: {limit}\\n\")\n",
    "    \n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedder.create_embedding(query).embedding\n",
    "    \n",
    "    # Perform vector search using the simplified API\n",
    "    # Include custom fields in output_fields\n",
    "    results = client.vectordb.search(\n",
    "        query_vector=query_embedding,\n",
    "        collection_name=collection_name,\n",
    "        limit=limit,\n",
    "        output_fields=[\"source\", \"offset\", \"filename\", \"catalog_urn\", \"model_name\", \"chunk_text\", \"chunk_index\", \"chunk_size\"]\n",
    "    )\n",
    "    \n",
    "    # The search returns a list directly, not an object with .results\n",
    "    print(f\"Found {len(results)} relevant chunks:\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        # Each result is likely a dict or object with metadata\n",
    "        # Let's check the structure\n",
    "        if hasattr(result, 'metadata'):\n",
    "            metadata = result.metadata\n",
    "        elif isinstance(result, dict) and 'metadata' in result:\n",
    "            metadata = result['metadata']\n",
    "        else:\n",
    "            metadata = {}\n",
    "            \n",
    "        # Get score\n",
    "        score = result.score if hasattr(result, 'score') else result.get('score', 0.0) if isinstance(result, dict) else 0.0\n",
    "        \n",
    "        # Get chunk text from metadata\n",
    "        chunk_text = metadata.get('chunk_text', None)\n",
    "        \n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"  üìä Score: {score:.4f}\")\n",
    "        print(f\"  üìÑ Source: {metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"  üìÅ Filename: {metadata.get('filename', 'Unknown')}\")\n",
    "        print(f\"  üìç Chunk Index: {metadata.get('chunk_index', metadata.get('offset', 'N/A'))}\")\n",
    "        print(f\"  ü§ñ Model: {metadata.get('model_name', 'N/A')}\")\n",
    "        print(f\"  üîó Catalog URN: {metadata.get('catalog_urn', 'N/A')}\")\n",
    "        print(f\"  üìè Chunk Size: {metadata.get('chunk_size', 'N/A')} chars\")\n",
    "        \n",
    "        # Display chunk text\n",
    "        if chunk_text:\n",
    "            print(f\"  üìù Content:\")\n",
    "            # Indent the chunk text\n",
    "            for line in chunk_text.split('\\n'):\n",
    "                print(f\"     {line}\")\n",
    "        else:\n",
    "            print(f\"  üìù Note: Chunk text not available in metadata\")\n",
    "            \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for: 'What are the conditions for transitioning planning to execution?'\n",
      "   - Collection: TestRAG3\n",
      "   - Max results: 3\n",
      "\n",
      "Found 3 relevant chunks:\n",
      "\n",
      "Result 1:\n",
      "  üìä Score: 0.5353\n",
      "  üìÑ Source: /home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md\n",
      "  üìÅ Filename: kz_info.md\n",
      "  üìç Chunk Index: 6\n",
      "  ü§ñ Model: BAAI/bge-base-en-v1.5\n",
      "  üîó Catalog URN: urn:li:dataset:(urn:li:dataPlatform:file,/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md,PROD)\n",
      "  üìè Chunk Size: 1991 chars\n",
      "  üìù Content:\n",
      "     unless explicitly specified in the deployment request:\n",
      "     \n",
      "     ```python\n",
      "     # 1. Explicit engine specification (highest priority)\n",
      "     if deployment_request.engine_name:\n",
      "         engine_name = deployment_request.engine_name\n",
      "     else:\n",
      "         # 2. Automatic detection based on platform, hardware, and model format\n",
      "         engine_name = determine_engine_automatically()\n",
      "     ```\n",
      "     \n",
      "     #### Automatic Engine Selection Algorithm\n",
      "     \n",
      "     ```python\n",
      "     def determine_engine_automatically():\n",
      "         # Platform and hardware detection\n",
      "         current_os = platform.system()  # \"Darwin\", \"Linux\", etc.\n",
      "         has_gpus = runtime_config.get_config('has_gpus')\n",
      "         is_ampere = runtime_config.get_config('is_ampere')\n",
      "         file_extensions = detect_model_file_extensions()  # .gguf, .safetensors, etc.\n",
      "         \n",
      "         if current_os == \"Darwin\":  # macOS\n",
      "             if \".gguf\" in file_extensions:\n",
      "                 return 'llamacpp'  # Optimized for Apple Silicon\n",
      "             elif \".safetensors\" in fi...\n",
      "--------------------------------------------------------------------------------\n",
      "Result 2:\n",
      "  üìä Score: 0.5346\n",
      "  üìÑ Source: /home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md\n",
      "  üìÅ Filename: kz_info.md\n",
      "  üìç Chunk Index: 3\n",
      "  ü§ñ Model: BAAI/bge-base-en-v1.5\n",
      "  üîó Catalog URN: urn:li:dataset:(urn:li:dataPlatform:file,/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md,PROD)\n",
      "  üìè Chunk Size: 1961 chars\n",
      "  üìù Content:\n",
      "     management\n",
      "     \n",
      "     #### üéØ Kamiwaza Services Layer\n",
      "     Core business logic and API services:\n",
      "     - **FastAPI Service Gateway**: Central API coordination\n",
      "     - **Authentication/Authorization**: Identity and access management\n",
      "     - **Model Repository**: Model storage and versioning\n",
      "     - **Model Serving**: AI model deployment and inference\n",
      "     - **Catalog Services**: Data discovery and lineage\n",
      "     \n",
      "     #### üìö Extended Services\n",
      "     Specialized services for advanced functionality:\n",
      "     - **Vector Databases**: High-dimensional data storage (Milvus, Qdrant)\n",
      "     - **Prompts Storage**: Template and prompt management\n",
      "     - **Model Cache**: Performance optimization\n",
      "     - **DataHub EE**: Enterprise data catalog\n",
      "     - **Community Catalog**: Shared resource discovery\n",
      "     \n",
      "     #### üåê Application Layer\n",
      "     User-facing applications and interfaces:\n",
      "     - **App Garden**: Containerized application deployment platform\n",
      "     - **Kamiwaza Frontend**: Web-based user interface\n",
      "     - **Client SDK**: P...\n",
      "--------------------------------------------------------------------------------\n",
      "Result 3:\n",
      "  üìä Score: 0.5105\n",
      "  üìÑ Source: /home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md\n",
      "  üìÅ Filename: kz_info.md\n",
      "  üìç Chunk Index: 0\n",
      "  ü§ñ Model: BAAI/bge-base-en-v1.5\n",
      "  üîó Catalog URN: urn:li:dataset:(urn:li:dataPlatform:file,/home/kamiwaza/kamiwaza-040/notebooks/sdk-rag/kz_info.md,PROD)\n",
      "  üìè Chunk Size: 1596 chars\n",
      "  üìù Content:\n",
      "     # Kamiwaza.AI Platform Architecture (Deep Dive)\n",
      "     \n",
      "     > **Heads-up üëã** ‚Äî For the quick-start, repository layout, and environment setup, read `docs/README.md`.  \n",
      "     > This document focuses purely on **architecture details** and omits getting-started instructions that duplicate the README.\n",
      "     \n",
      "     ## Table of Contents\n",
      "     \n",
      "     1. [System Architecture](#system-architecture)\n",
      "     2. [Core Components](#core-components)\n",
      "     3. [Model Serving Engines](#model-serving-engines)\n",
      "     4. [Technology Stack](#technology-stack)\n",
      "     5. [Service Layer](#service-layer)\n",
      "     6. [Deployment Architecture](#deployment-architecture)\n",
      "     7. [App Garden](#app-garden)\n",
      "     8. [Data Layer](#data-layer)\n",
      "     9. [Related Documentation](#related-documentation)\n",
      "     \n",
      "     ---\n",
      "     \n",
      "     ## System Architecture\n",
      "     \n",
      "     Kamiwaza follows a layered architecture pattern, with each layer providing specific capabilities and abstractions:\n",
      "     \n",
      "     ```mermaid\n",
      "     flowchart BT\n",
      "         %% Color definitions\n",
      "         classDef infraColor ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîç Searching for: 'How does the operational art inform the development of a campaign plan?'\n",
      "   - Collection: TestRAG3\n",
      "   - Max results: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example searches\n",
    "queries = [\n",
    "    \"What are the conditions for transitioning planning to execution?\",\n",
    "    \"How does the operational art inform the development of a campaign plan?\",\n",
    "    \"What is a ‚ÄúCenter of Gravity‚Äù in operational design?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    results = semantic_search(query, COLLECTION_NAME, limit=3)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collection Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Available Collections:\n",
      "   - JointPlanningProcess\n",
      "   - KamiDocs\n"
     ]
    }
   ],
   "source": [
    "# List all collections\n",
    "def list_collections():\n",
    "    \"\"\"List all vector collections in the database.\"\"\"\n",
    "    collections = client.vectordb.list_collections()\n",
    "    \n",
    "    print(\"üìö Available Collections:\")\n",
    "    for collection in collections:\n",
    "        print(f\"   - {collection}\")\n",
    "    \n",
    "    return collections\n",
    "\n",
    "collections = list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean up resources\n",
    "def cleanup_collection(collection_name: str):\n",
    "    \"\"\"\n",
    "    Drop a vector collection.\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the collection to drop\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.vectordb.drop_collection(collection_name)\n",
    "        print(f\"‚úÖ Dropped collection: {collection_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error dropping collection: {str(e)}\")\n",
    "\n",
    "# Uncomment to clean up\n",
    "# cleanup_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete RAG workflow using Kamiwaza SDK:\n",
    "\n",
    "1. **Catalog Integration**: Documents are first uploaded to the catalog for centralized management\n",
    "2. **Chunking**: Documents are split into overlapping chunks for better context\n",
    "3. **Embedding**: Each chunk is converted to a vector representation\n",
    "4. **Storage**: Vectors are stored with chunk text as metadata for instant retrieval\n",
    "5. **Search**: Semantic search returns relevant chunks with their full text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

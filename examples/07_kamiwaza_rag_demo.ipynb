{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kamiwaza RAG Demo: Document Processing with Catalog Integration\n",
    "\n",
    "This notebook demonstrates the complete RAG (Retrieval-Augmented Generation) workflow using Kamiwaza SDK:\n",
    "1. Upload documents to the catalog\n",
    "2. Chunk documents into manageable pieces\n",
    "3. Generate embeddings for each chunk\n",
    "4. Store vectors with chunk text as metadata \n",
    "5. Perform semantic search on the content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Kamiwaza at: http://localhost:7777/api/\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# Import Kamiwaza SDK\n",
    "from kamiwaza_client import KamiwazaClient\n",
    "from kamiwaza_client.schemas.catalog import Dataset\n",
    "from kamiwaza_client.schemas.vectordb import (\n",
    "    InsertVectorsRequest,\n",
    "    SearchVectorsRequest,\n",
    "    SearchResult\n",
    ")\n",
    "\n",
    "# Initialize the Kamiwaza client\n",
    "KAMIWAZA_API_URL = \"http://localhost:7777/api/\"\n",
    "client = KamiwazaClient(base_url=KAMIWAZA_API_URL)\n",
    "\n",
    "print(f\"Connected to Kamiwaza at: {KAMIWAZA_API_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Upload to Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to add files to catalog (based on the example)\n",
    "def add_files_to_catalog(filepaths, client, platform=\"file\", recursive=True, description=\"\"):\n",
    "    \"\"\"\n",
    "    Add files to the Kamiwaza catalog.\n",
    "    \n",
    "    Args:\n",
    "        filepaths: List of file paths or a single file path\n",
    "        client: KamiwazaClient instance\n",
    "        platform: Platform identifier (default: \"file\")\n",
    "        recursive: Whether to process directories recursively\n",
    "        description: Description for the datasets\n",
    "    \n",
    "    Returns:\n",
    "        List of URNs for created datasets\n",
    "    \"\"\"\n",
    "    if isinstance(filepaths, str):\n",
    "        filepaths = [filepaths]\n",
    "    \n",
    "    urns = []\n",
    "    datasets = []  # Keep track of dataset objects\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        try:\n",
    "            # Create dataset for each file\n",
    "            dataset = client.catalog.create_dataset(\n",
    "                dataset_name=filepath,\n",
    "                platform=platform,\n",
    "                environment=\"PROD\",\n",
    "                description=description or f\"File: {Path(filepath).name}\"\n",
    "            )\n",
    "            \n",
    "            if dataset.urn:\n",
    "                urns.append(dataset.urn)\n",
    "                datasets.append(dataset)  # Store the dataset object\n",
    "                print(f\"✅ Added to catalog: {Path(filepath).name}\")\n",
    "                print(f\"   URN: {dataset.urn}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error adding {filepath}: {str(e)}\")\n",
    "    \n",
    "    # Return both URNs and datasets\n",
    "    return urns, datasets\n",
    "\n",
    "# Function to show dataset info\n",
    "def show_dataset_info(client, urns):\n",
    "    \"\"\"Display information about datasets in the catalog.\"\"\"\n",
    "    all_datasets = client.catalog.list_datasets()\n",
    "    my_datasets = [d for d in all_datasets if d.urn in urns]\n",
    "    \n",
    "    print(f\"Total datasets in catalog: {len(all_datasets)}\")\n",
    "    print(f\"Matching datasets: {len(my_datasets)}\")\n",
    "    \n",
    "    for d in my_datasets:\n",
    "        print(f\"\\nURN: {d.urn}\")\n",
    "        print(f\"ID: {d.id}\")\n",
    "        print(f\"Platform: {d.platform}\")\n",
    "        print(f\"Environment: {d.environment}\")\n",
    "        print(f\"Name: {d.name if d.name else 'None'}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return my_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added to catalog: kz_marketing.md\n",
      "   URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md,PROD)\n",
      "✅ Added to catalog: kz_tech_info.md\n",
      "   URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_tech_info.md,PROD)\n",
      "\n",
      "Created 2 datasets in catalog\n",
      "\n",
      "Dataset Information from Catalog:\n",
      "Total datasets in catalog: 2\n",
      "Matching datasets: 2\n",
      "\n",
      "URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md,PROD)\n",
      "ID: /Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md\n",
      "Platform: file\n",
      "Environment: PROD\n",
      "Name: None\n",
      "--------------------------------------------------\n",
      "\n",
      "URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_tech_info.md,PROD)\n",
      "ID: /Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_tech_info.md\n",
      "Platform: file\n",
      "Environment: PROD\n",
      "Name: None\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example: Upload documents to catalog\n",
    "# Replace with your document paths\n",
    "DOCUMENT_PATHS = [\n",
    "    \"./kz_marketing.md\",  \n",
    "    \"./kz_tech_info.md\"\n",
    "    # Add more documents as needed\n",
    "]\n",
    "\n",
    "# Add files to catalog - now returns both URNs and datasets\n",
    "urns, datasets_created = add_files_to_catalog(\n",
    "    filepaths=DOCUMENT_PATHS,\n",
    "    client=client,\n",
    "    platform=\"file\",\n",
    "    description=\"RAG documents\"\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(urns)} datasets in catalog\")\n",
    "\n",
    "# Show dataset information from list_datasets\n",
    "if urns:\n",
    "    print(\"\\nDataset Information from Catalog:\")\n",
    "    datasets_from_catalog = show_dataset_info(client, urns)\n",
    "    \n",
    "    # Use the datasets we created directly if list_datasets doesn't return them\n",
    "    if not datasets_from_catalog and datasets_created:\n",
    "        print(\"\\nUsing created datasets directly:\")\n",
    "        datasets = datasets_created\n",
    "        for d in datasets:\n",
    "            print(f\"\\nURN: {d.urn}\")\n",
    "            print(f\"ID: {d.id}\")\n",
    "            print(f\"Platform: {d.platform}\")\n",
    "            print(f\"Environment: {d.environment}\")\n",
    "            print(f\"Name: {d.name if d.name else 'None'}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        datasets = datasets_from_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Chunking and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking configuration:\n",
      "  - Chunk size: 600 tokens\n",
      "  - Overlap: 102 tokens\n",
      "  - Embedding model: BAAI/bge-base-en-v1.5\n",
      "  - Collection name: TestRAG_1754662907\n",
      "\n",
      "Note: Using a new collection to ensure clean schema\n",
      "\n",
      "Initializing global embedder...\n",
      "✅ Global embedder initialized and ready\n"
     ]
    }
   ],
   "source": [
    "# Configuration for chunking and embedding\n",
    "CHUNK_SIZE = 600\n",
    "CHUNK_OVERLAP = 102  # Token overlap between chunks\n",
    "EMBEDDER_MODEL = \"BAAI/bge-base-en-v1.5\" \n",
    "\n",
    "# Use a unique collection name to avoid schema conflicts\n",
    "import time\n",
    "timestamp = int(time.time())\n",
    "COLLECTION_NAME = f\"TestRAG_{timestamp}\"\n",
    "\n",
    "print(f\"Chunking configuration:\")\n",
    "print(f\"  - Chunk size: {CHUNK_SIZE} tokens\")\n",
    "print(f\"  - Overlap: {CHUNK_OVERLAP} tokens\")\n",
    "print(f\"  - Embedding model: {EMBEDDER_MODEL}\")\n",
    "print(f\"  - Collection name: {COLLECTION_NAME}\")\n",
    "print(f\"\\nNote: Using a new collection to ensure clean schema\")\n",
    "\n",
    "# Ensure a fresh collection schema if a name collision occurs\n",
    "existing_collections = client.vectordb.list_collections()\n",
    "if COLLECTION_NAME in existing_collections:\n",
    "    print(f\"\\nDropping existing collection '{COLLECTION_NAME}' to ensure clean schema\")\n",
    "    client.vectordb.drop_collection(COLLECTION_NAME)\n",
    "\n",
    "# Create a global embedder instance to prevent cleanup between operations\n",
    "print(f\"\\nInitializing global embedder...\")\n",
    "GLOBAL_EMBEDDER = client.embedding.get_embedder(\n",
    "    model=EMBEDDER_MODEL,\n",
    "    provider_type=\"huggingface_embedding\"\n",
    ")\n",
    "print(f\"✅ Global embedder initialized and ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_utf8_bytes(text: str, max_bytes: int = 1000, suffix: str = \"...\") -> str:\n",
    "    \"\"\"Truncate a string so its UTF-8 byte length <= max_bytes (including suffix).\"\"\"\n",
    "    raw = text.encode(\"utf-8\")\n",
    "    if len(raw) <= max_bytes:\n",
    "        return text\n",
    "    suffix_bytes = suffix.encode(\"utf-8\")\n",
    "    limit = max(0, max_bytes - len(suffix_bytes))\n",
    "    candidate = raw[:limit]\n",
    "    # Ensure we don't cut a multi-byte character in the middle\n",
    "    while True:\n",
    "        try:\n",
    "            head = candidate.decode(\"utf-8\")\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            candidate = candidate[:-1]\n",
    "            if not candidate:\n",
    "                head = \"\"\n",
    "                break\n",
    "    return head + suffix\n",
    "\n",
    "\n",
    "def process_document_from_catalog(dataset: Dataset, collection_name: str, embedder=None):\n",
    "    \"\"\"\n",
    "    Process a document from the catalog: chunk, embed, and store in vector DB.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The catalog dataset containing the document\n",
    "        collection_name: Name of the vector collection to store chunks\n",
    "        embedder: Optional embedder instance (uses GLOBAL_EMBEDDER if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        Number of chunks processed\n",
    "    \"\"\"\n",
    "    # Use provided embedder or global instance\n",
    "    if embedder is None:\n",
    "        embedder = GLOBAL_EMBEDDER\n",
    "    \n",
    "    # Extract file path from dataset\n",
    "    # The dataset id contains the file path\n",
    "    doc_path = Path(dataset.id)\n",
    "    \n",
    "    # Fallback: Parse path from URN if needed\n",
    "    if not doc_path.exists() and dataset.urn:\n",
    "        # URN format: urn:li:dataset:(urn:li:dataPlatform:file,/path/to/file,PROD)\n",
    "        parts = dataset.urn.split(',')\n",
    "        if len(parts) >= 2:\n",
    "            doc_path = Path(parts[1])\n",
    "    \n",
    "    if not doc_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {doc_path}\")\n",
    "    \n",
    "    # Read document content\n",
    "    with open(doc_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    print(f\"📄 Processing document: {doc_path.name}\")\n",
    "    print(f\"   - Size: {len(content)} characters\")\n",
    "    print(f\"   - URN: {dataset.urn}\")\n",
    "    \n",
    "    try:\n",
    "        # Chunk the document\n",
    "        print(f\"\\n📋 Chunking document...\")\n",
    "        chunks = embedder.chunk_text(\n",
    "            text=content,\n",
    "            max_length=CHUNK_SIZE,\n",
    "            overlap=CHUNK_OVERLAP\n",
    "        )\n",
    "        \n",
    "        print(f\"   - Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Generate embeddings for all chunks\n",
    "        print(f\"\\n🧮 Generating embeddings...\")\n",
    "        embeddings = embedder.embed_chunks(chunks)\n",
    "        \n",
    "        # Prepare metadata for each chunk\n",
    "        # Include both autofields and custom fields\n",
    "        metadata_list = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Truncate chunk text to respect Milvus VARCHAR 1000-byte limit (UTF-8)\n",
    "            chunk_text = truncate_utf8_bytes(chunk, max_bytes=1000, suffix=\"...\")\n",
    "            # Debug: print UTF-8 byte length to verify\n",
    "            print(len(chunk_text.encode('utf-8')))\n",
    "            \n",
    "            metadata = {\n",
    "                # Required autofields (these MUST be included)\n",
    "                \"model_name\": EMBEDDER_MODEL,\n",
    "                \"source\": str(doc_path),\n",
    "                \"catalog_urn\": dataset.urn or \"\",\n",
    "                \"offset\": i,\n",
    "                \"filename\": doc_path.name,\n",
    "                \n",
    "                # Custom fields - these will be added to the schema\n",
    "                \"chunk_text\": chunk_text,\n",
    "                \"chunk_index\": i,\n",
    "                \"chunk_size\": len(chunk),\n",
    "            }\n",
    "            metadata_list.append(metadata)\n",
    "        \n",
    "        # Insert vectors into the database\n",
    "        print(f\"\\n💾 Inserting vectors into collection '{collection_name}'...\")\n",
    "        \n",
    "        # Define custom fields for the collection schema\n",
    "        # IMPORTANT: Use tuple format (field_name, field_type)\n",
    "        field_list = [\n",
    "            (\"chunk_text\", \"str\"),      # Store the actual chunk text\n",
    "            (\"chunk_index\", \"int\"),     # Store chunk index\n",
    "            (\"chunk_size\", \"int\"),      # Store chunk size\n",
    "        ]\n",
    "        \n",
    "        # Use the SDK's insert method with field_list\n",
    "        response = client.vectordb.insert(\n",
    "            vectors=embeddings,\n",
    "            metadata=metadata_list,\n",
    "            collection_name=collection_name,\n",
    "            field_list=field_list  # Pass custom fields\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Successfully inserted {len(chunks)} vectors\")\n",
    "        print(f\"   - Collection: {collection_name}\")\n",
    "        print(f\"   - Custom fields added: chunk_text, chunk_index, chunk_size\")\n",
    "        print(f\"   - Chunk text stored directly in Milvus!\")\n",
    "        \n",
    "        return len(chunks)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during processing: {str(e)}\")\n",
    "        print(f\"\\nDebug info:\")\n",
    "        print(f\"  - Collection name: {collection_name}\")\n",
    "        print(f\"  - Number of vectors: {len(embeddings) if 'embeddings' in locals() else 'N/A'}\")\n",
    "        print(f\"  - Number of metadata entries: {len(metadata_list) if 'metadata_list' in locals() else 'N/A'}\")\n",
    "        print(f\"  - Field list: {field_list if 'field_list' in locals() else 'N/A'}\")\n",
    "        if 'metadata_list' in locals() and metadata_list:\n",
    "            print(f\"  - Sample metadata keys: {list(metadata_list[0].keys())}\")\n",
    "        raise\n",
    "\n",
    "# Function to process all datasets from catalog\n",
    "def process_catalog_datasets(datasets, collection_name):\n",
    "    \"\"\"Process multiple datasets from the catalog.\"\"\"\n",
    "    total_chunks = 0\n",
    "    \n",
    "    # Use the global embedder for all datasets\n",
    "    for dataset in datasets:\n",
    "        print(dataset)\n",
    "        try:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            chunks = process_document_from_catalog(dataset, collection_name, embedder=GLOBAL_EMBEDDER)\n",
    "            total_chunks += chunks\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing dataset {dataset.id}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n🎉 Total chunks processed: {total_chunks}\")\n",
    "    return total_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urn='urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md,PROD)' id='/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md' platform='file' environment='PROD' paths=None name=None actor=None customProperties=None removed=None tags=None\n",
      "\n",
      "================================================================================\n",
      "📄 Processing document: kz_marketing.md\n",
      "   - Size: 10081 characters\n",
      "   - URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md,PROD)\n",
      "\n",
      "📋 Chunking document...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 10:21:55,202 - kamiwaza_client.services.embedding - INFO - Starting embedding generation for 6 chunks (batch size: 64)\n",
      "2025-08-08 10:21:55,312 - kamiwaza_client.services.embedding - INFO - Successfully generated embeddings for 6 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Created 6 chunks\n",
      "\n",
      "🧮 Generating embeddings...\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "\n",
      "💾 Inserting vectors into collection 'TestRAG_1754662907'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-08 10:21:56,435 - kamiwaza_client.services.embedding - INFO - Starting embedding generation for 7 chunks (batch size: 64)\n",
      "2025-08-08 10:21:56,574 - kamiwaza_client.services.embedding - INFO - Successfully generated embeddings for 7 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully inserted 6 vectors\n",
      "   - Collection: TestRAG_1754662907\n",
      "   - Custom fields added: chunk_text, chunk_index, chunk_size\n",
      "   - Chunk text stored directly in Milvus!\n",
      "urn='urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_tech_info.md,PROD)' id='/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_tech_info.md' platform='file' environment='PROD' paths=None name=None actor=None customProperties=None removed=None tags=None\n",
      "\n",
      "================================================================================\n",
      "📄 Processing document: kz_tech_info.md\n",
      "   - Size: 11793 characters\n",
      "   - URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_tech_info.md,PROD)\n",
      "\n",
      "📋 Chunking document...\n",
      "   - Created 7 chunks\n",
      "\n",
      "🧮 Generating embeddings...\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "\n",
      "💾 Inserting vectors into collection 'TestRAG_1754662907'...\n",
      "✅ Successfully inserted 7 vectors\n",
      "   - Collection: TestRAG_1754662907\n",
      "   - Custom fields added: chunk_text, chunk_index, chunk_size\n",
      "   - Chunk text stored directly in Milvus!\n",
      "\n",
      "🎉 Total chunks processed: 13\n"
     ]
    }
   ],
   "source": [
    "# Process documents from catalog\n",
    "if datasets:\n",
    "    total_chunks = process_catalog_datasets(datasets, COLLECTION_NAME)\n",
    "else:\n",
    "    print(\"No datasets found in catalog to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, collection_name: str, limit: int = 5, embedder=None):\n",
    "    \"\"\"\n",
    "    Perform semantic search on the document collection.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        collection_name: Name of the vector collection\n",
    "        limit: Maximum number of results to return\n",
    "        embedder: Optional embedder instance (uses GLOBAL_EMBEDDER if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        Search results with chunk text and metadata\n",
    "    \"\"\"\n",
    "    # Use provided embedder or global instance\n",
    "    if embedder is None:\n",
    "        embedder = GLOBAL_EMBEDDER\n",
    "        \n",
    "    print(f\"🔍 Searching for: '{query}'\")\n",
    "    print(f\"   - Collection: {collection_name}\")\n",
    "    print(f\"   - Max results: {limit}\\n\")\n",
    "    \n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedder.create_embedding(query).embedding\n",
    "    \n",
    "    # Perform vector search using the simplified API\n",
    "    # Include custom fields in output_fields\n",
    "    results = client.vectordb.search(\n",
    "        query_vector=query_embedding,\n",
    "        collection_name=collection_name,\n",
    "        limit=limit,\n",
    "        output_fields=[\"source\", \"offset\", \"filename\", \"catalog_urn\", \"model_name\", \"chunk_text\", \"chunk_index\", \"chunk_size\"]\n",
    "    )\n",
    "    \n",
    "    # The search returns a list directly, not an object with .results\n",
    "    print(f\"Found {len(results)} relevant chunks:\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        # Each result is likely a dict or object with metadata\n",
    "        # Let's check the structure\n",
    "        if hasattr(result, 'metadata'):\n",
    "            metadata = result.metadata\n",
    "        elif isinstance(result, dict) and 'metadata' in result:\n",
    "            metadata = result['metadata']\n",
    "        else:\n",
    "            metadata = {}\n",
    "            \n",
    "        # Get score\n",
    "        score = result.score if hasattr(result, 'score') else result.get('score', 0.0) if isinstance(result, dict) else 0.0\n",
    "        \n",
    "        # Get chunk text from metadata\n",
    "        chunk_text = metadata.get('chunk_text', None)\n",
    "        \n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"  📊 Score: {score:.4f}\")\n",
    "        print(f\"  📄 Source: {metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"  📁 Filename: {metadata.get('filename', 'Unknown')}\")\n",
    "        print(f\"  📍 Chunk Index: {metadata.get('chunk_index', metadata.get('offset', 'N/A'))}\")\n",
    "        print(f\"  🤖 Model: {metadata.get('model_name', 'N/A')}\")\n",
    "        print(f\"  🔗 Catalog URN: {metadata.get('catalog_urn', 'N/A')}\")\n",
    "        print(f\"  📏 Chunk Size: {metadata.get('chunk_size', 'N/A')} chars\")\n",
    "        \n",
    "        # Display chunk text\n",
    "        if chunk_text:\n",
    "            print(f\"  📝 Content:\")\n",
    "            # Indent the chunk text\n",
    "            for line in chunk_text.split('\\n'):\n",
    "                print(f\"     {line}\")\n",
    "        else:\n",
    "            print(f\"  📝 Note: Chunk text not available in metadata\")\n",
    "            \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Searching for: 'How does Kamiwaza benefit the enterprise?'\n",
      "   - Collection: TestRAG_1754662907\n",
      "   - Max results: 3\n",
      "\n",
      "Found 3 relevant chunks:\n",
      "\n",
      "Result 1:\n",
      "  📊 Score: 0.7176\n",
      "  📄 Source: /Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md\n",
      "  📁 Filename: kz_marketing.md\n",
      "  📍 Chunk Index: 0\n",
      "  🤖 Model: BAAI/bge-base-en-v1.5\n",
      "  🔗 Catalog URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md,PROD)\n",
      "  📏 Chunk Size: 2219 chars\n",
      "  📝 Content:\n",
      "     # Kamiwaza Marketing Team Onboarding Guide\n",
      "     \n",
      "     ## Welcome to Kamiwaza! 🚀\n",
      "     \n",
      "     ### The Elevator Pitch\n",
      "     **Kamiwaza is the enterprise AI platform that makes deploying and managing AI models as simple as using traditional software.** We eliminate the complexity of AI infrastructure, allowing organizations to focus on innovation rather than operations.\n",
      "     \n",
      "     ---\n",
      "     \n",
      "     ## What is Kamiwaza?\n",
      "     \n",
      "     Kamiwaza is an **enterprise-grade AI/ML platform** that provides everything organizations need to deploy, serve, and manage AI models at scale. Think of it as the \"operating system\" for AI in your enterprise.\n",
      "     \n",
      "     ### The Problem We Solve\n",
      "     \n",
      "     **Before Kamiwaza:**\n",
      "     - Companies struggle to move AI models from experimentation to production\n",
      "     - Data scientists spend 80% of their time on infrastructure, not innovation\n",
      "     - Different teams use incompatible tools and frameworks\n",
      "     - GPU resources are underutilized and expensive\n",
      "     - Security and compliance are afterthoughts\n",
      "     \n",
      "     **With Kamiwaza:**\n",
      "     - Deploy any AI model in minutes, not months\n",
      "     - Da...\n",
      "--------------------------------------------------------------------------------\n",
      "Result 2:\n",
      "  📊 Score: 0.6901\n",
      "  📄 Source: /Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md\n",
      "  📁 Filename: kz_marketing.md\n",
      "  📍 Chunk Index: 4\n",
      "  🤖 Model: BAAI/bge-base-en-v1.5\n",
      "  🔗 Catalog URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md,PROD)\n",
      "  📏 Chunk Size: 2058 chars\n",
      "  📝 Content:\n",
      "     \"\n",
      "     - \"Reduce AI infrastructure costs by 70%\"\n",
      "     - \"Enterprise AI that actually works in the enterprise\"\n",
      "     \n",
      "     ### **For Developers:**\n",
      "     - \"Deploy models as easily as deploying web apps\"\n",
      "     - \"One API for all your AI models\"\n",
      "     - \"Stop managing infrastructure, start building features\"\n",
      "     \n",
      "     ---\n",
      "     \n",
      "     ## Objection Handling\n",
      "     \n",
      "     ### **\"We already use cloud AI services\"**\n",
      "     - Kamiwaza complements cloud services by bringing AI on-premise\n",
      "     - Reduce costs for high-volume workloads\n",
      "     - Maintain data sovereignty and compliance\n",
      "     \n",
      "     ### **\"We built our own ML platform\"**\n",
      "     - Kamiwaza can integrate with existing systems\n",
      "     - Reduce maintenance burden on your team\n",
      "     - Get enterprise features without building them\n",
      "     \n",
      "     ### **\"AI is too complex for us\"**\n",
      "     - Kamiwaza makes AI as simple as traditional software\n",
      "     - Start with pre-trained models, no ML expertise required\n",
      "     - Our community edition is free to try\n",
      "     \n",
      "     ### **\"We don't have GPUs\"**\n",
      "     - Kamiwaza works with CPU-only deployments\n",
      "     - Optimize existing hardware before buying more\n",
      "     - Support for all GPU ven...\n",
      "--------------------------------------------------------------------------------\n",
      "Result 3:\n",
      "  📊 Score: 0.6715\n",
      "  📄 Source: /Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md\n",
      "  📁 Filename: kz_marketing.md\n",
      "  📍 Chunk Index: 2\n",
      "  🤖 Model: BAAI/bge-base-en-v1.5\n",
      "  🔗 Catalog URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md,PROD)\n",
      "  📏 Chunk Size: 1945 chars\n",
      "  📝 Content:\n",
      "     manage and monitor\n",
      "     - Enterprise-grade security built-in\n",
      "     - Automated resource optimization\n",
      "     \n",
      "     **Key Message:** \"One platform to rule them all\"\n",
      "     \n",
      "     ### 3. **Business Leaders/CTOs**\n",
      "     **Pain Points:**\n",
      "     - Slow AI adoption and ROI\n",
      "     - High infrastructure costs\n",
      "     - Risk and compliance concerns\n",
      "     \n",
      "     **How Kamiwaza Helps:**\n",
      "     - Accelerate AI initiatives\n",
      "     - Reduce infrastructure costs by 70%\n",
      "     - Enterprise security and compliance\n",
      "     \n",
      "     **Key Message:** \"Accelerate AI adoption while reducing risk and cost\"\n",
      "     \n",
      "     ---\n",
      "     \n",
      "     ## Key Features to Highlight\n",
      "     \n",
      "     ### 🎯 **Model Hub Integration**\n",
      "     - \"Access thousands of pre-trained models from HuggingFace with one click\"\n",
      "     - \"No need to manage model files or versions manually\"\n",
      "     \n",
      "     ### 🚀 **Multi-Engine Support**\n",
      "     - \"Choose the best engine for your use case - VLLM for speed, LlamaCpp for efficiency\"\n",
      "     - \"Switch between engines without changing your code\"\n",
      "     \n",
      "     ### 🔄 **Intelligent Resource Management**\n",
      "     - \"Automatically allocate GPUs based on model requirements\"\n",
      "     - \"Run multiple models on a single GPU...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "\n",
      "🔍 Searching for: 'Can Kamiwaza deploy on any hardware?'\n",
      "   - Collection: TestRAG_1754662907\n",
      "   - Max results: 3\n",
      "\n",
      "Found 3 relevant chunks:\n",
      "\n",
      "Result 1:\n",
      "  📊 Score: 0.7464\n",
      "  📄 Source: /Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_tech_info.md\n",
      "  📁 Filename: kz_tech_info.md\n",
      "  📍 Chunk Index: 2\n",
      "  🤖 Model: BAAI/bge-base-en-v1.5\n",
      "  🔗 Catalog URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_tech_info.md,PROD)\n",
      "  📏 Chunk Size: 1587 chars\n",
      "  📝 Content:\n",
      "     install.sh --community --i-accept-the-kamiwaza-license\n",
      "     ```\n",
      "     \n",
      "     ### Multi-Node Production\n",
      "     ```bash\n",
      "     # Head node with full stack\n",
      "     bash install.sh --head --i-accept-the-kamiwaza-license\n",
      "     \n",
      "     # Worker nodes for additional compute\n",
      "     export KAMIWAZA_HEAD_IP=<head-ip>\n",
      "     bash install.sh --worker --i-accept-the-kamiwaza-license\n",
      "     ```\n",
      "     \n",
      "     ### Container Architecture\n",
      "     - **Multi-architecture support**: AMD64/ARM64 containers\n",
      "     - **GPU-aware scheduling**: Automatic container selection based on hardware\n",
      "     - **Resource isolation**: cgroups and namespace isolation\n",
      "     \n",
      "     ## Model Deployment Workflow\n",
      "     \n",
      "     ### 1. Model Acquisition\n",
      "     ```python\n",
      "     # Download from HuggingFace or custom repositories\n",
      "     POST /api/models/download\n",
      "     {\n",
      "         \"repo_id\": \"meta-llama/Llama-2-70b-hf\",\n",
      "         \"revision\": \"main\"\n",
      "     }\n",
      "     ```\n",
      "     \n",
      "     ### 2. Configuration Management\n",
      "     ```python\n",
      "     # Create deployment configuration\n",
      "     POST /api/models/{model_id}/configs\n",
      "     {\n",
      "         \"name\": \"production-config\",\n",
      "         \"config\": {\n",
      "             \"max_model_len\": 4096,\n",
      "             \"tensor_parallel_size\": 4,\n",
      "             \"gpu_memory...\n",
      "--------------------------------------------------------------------------------\n",
      "Result 2:\n",
      "  📊 Score: 0.7241\n",
      "  📄 Source: /Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_tech_info.md\n",
      "  📁 Filename: kz_tech_info.md\n",
      "  📍 Chunk Index: 1\n",
      "  🤖 Model: BAAI/bge-base-en-v1.5\n",
      "  🔗 Catalog URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_tech_info.md,PROD)\n",
      "  📏 Chunk Size: 2056 chars\n",
      "  📝 Content:\n",
      "     LLM       │ • Milvus      │ • CockroachDB (SQL)      │\n",
      "     │ • LlamaCpp   │ • Qdrant      │ • etcd (Coordination)    │\n",
      "     │ • MLX        │               │ • Docker/Containers      │\n",
      "     └──────────────┴───────────────┴──────────────────────────┘\n",
      "     ```\n",
      "     \n",
      "     ### Distributed Computing Foundation\n",
      "     \n",
      "     **Ray Cluster**: At its heart, Kamiwaza leverages Ray for distributed computing, enabling:\n",
      "     - Automatic work distribution across nodes\n",
      "     - GPU memory management and allocation\n",
      "     - Fault tolerance with automatic recovery\n",
      "     - Dynamic scaling based on workload\n",
      "     \n",
      "     **Multi-Node Architecture**:\n",
      "     ```python\n",
      "     # Head Node: Runs all services + coordination\n",
      "     # Worker Nodes: Additional compute capacity\n",
      "     # Automatic node discovery via etcd\n",
      "     ```\n",
      "     \n",
      "     ### Inference Engine Abstraction\n",
      "     \n",
      "     Kamiwaza implements a sophisticated engine selection system:\n",
      "     \n",
      "     ```python\n",
      "     class EngineSelector:\n",
      "         def select_...\n",
      "--------------------------------------------------------------------------------\n",
      "Result 3:\n",
      "  📊 Score: 0.7187\n",
      "  📄 Source: /Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md\n",
      "  📁 Filename: kz_marketing.md\n",
      "  📍 Chunk Index: 0\n",
      "  🤖 Model: BAAI/bge-base-en-v1.5\n",
      "  🔗 Catalog URN: urn:li:dataset:(urn:li:dataPlatform:file,/Users/tylerhouchin/code/kamiwaza/notebooks/sdk/kz_marketing.md,PROD)\n",
      "  📏 Chunk Size: 2219 chars\n",
      "  📝 Content:\n",
      "     # Kamiwaza Marketing Team Onboarding Guide\n",
      "     \n",
      "     ## Welcome to Kamiwaza! 🚀\n",
      "     \n",
      "     ### The Elevator Pitch\n",
      "     **Kamiwaza is the enterprise AI platform that makes deploying and managing AI models as simple as using traditional software.** We eliminate the complexity of AI infrastructure, allowing organizations to focus on innovation rather than operations.\n",
      "     \n",
      "     ---\n",
      "     \n",
      "     ## What is Kamiwaza?\n",
      "     \n",
      "     Kamiwaza is an **enterprise-grade AI/ML platform** that provides everything organizations need to deploy, serve, and manage AI models at scale. Think of it as the \"operating system\" for AI in your enterprise.\n",
      "     \n",
      "     ### The Problem We Solve\n",
      "     \n",
      "     **Before Kamiwaza:**\n",
      "     - Companies struggle to move AI models from experimentation to production\n",
      "     - Data scientists spend 80% of their time on infrastructure, not innovation\n",
      "     - Different teams use incompatible tools and frameworks\n",
      "     - GPU resources are underutilized and expensive\n",
      "     - Security and compliance are afterthoughts\n",
      "     \n",
      "     **With Kamiwaza:**\n",
      "     - Deploy any AI model in minutes, not months\n",
      "     - Da...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example searches\n",
    "queries = [\n",
    "    \"How does Kamiwaza benefit the enterprise?\",\n",
    "    \"Can Kamiwaza deploy on any hardware?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    results = semantic_search(query, COLLECTION_NAME, limit=3)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collection Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Available Collections:\n",
      "   - TestRAG_1754662907\n",
      "   - TestRAG3\n",
      "   - KamiwazaRAGDemo\n"
     ]
    }
   ],
   "source": [
    "# List all collections\n",
    "def list_collections():\n",
    "    \"\"\"List all vector collections in the database.\"\"\"\n",
    "    collections = client.vectordb.list_collections()\n",
    "    \n",
    "    print(\"📚 Available Collections:\")\n",
    "    for collection in collections:\n",
    "        print(f\"   - {collection}\")\n",
    "    \n",
    "    return collections\n",
    "\n",
    "collections = list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dropped collection: KamiwazaRAGDemo\n"
     ]
    }
   ],
   "source": [
    "# Function to clean up resources\n",
    "def cleanup_collection(collection_name: str):\n",
    "    \"\"\"\n",
    "    Drop a vector collection.\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the collection to drop\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client.vectordb.drop_collection(collection_name)\n",
    "        print(f\"✅ Dropped collection: {collection_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error dropping collection: {str(e)}\")\n",
    "\n",
    "# Uncomment to clean up\n",
    "# cleanup_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete RAG workflow using Kamiwaza SDK:\n",
    "\n",
    "1. **Catalog Integration**: Documents are first uploaded to the catalog for centralized management\n",
    "2. **Chunking**: Documents are split into overlapping chunks for better context\n",
    "3. **Embedding**: Each chunk is converted to a vector representation\n",
    "4. **Storage**: Vectors are stored with chunk text as metadata for instant retrieval\n",
    "5. **Search**: Semantic search returns relevant chunks with their full text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
